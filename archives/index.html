
<!DOCTYPE html>
<html lang="zh-CN">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="hero576的博客">
    <title>Archiv - hero576的博客</title>
    <meta name="author" content="hero576">
    
        <meta name="keywords" content="py通红,">
    
    
    
    <script type="application/ld+json">{}</script>
    <meta name="description" content="并无特长">
<meta name="keywords" content="py通红">
<meta property="og:type" content="blog">
<meta property="og:title" content="hero576的博客">
<meta property="og:url" content="http://guoming576.cn/archives/index.html">
<meta property="og:site_name" content="hero576的博客">
<meta property="og:description" content="并无特长">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="hero576的博客">
<meta name="twitter:description" content="并无特长">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/all.css">
    <link rel="stylesheet" href="/assets/css/jquery.fancybox.css">
    <link rel="stylesheet" href="/assets/css/thumbs.css">
    <link rel="stylesheet" href="/assets/css/tranquilpeak.css">
    <!--STYLES END-->
    

    

    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="1">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/ "
            aria-label=""
        >
            hero576的博客
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Öffne den Link: /#about"
            >
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="1">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Kategorien"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Kategorien</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archiv"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archiv</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link open-algolia-search"
                            href="#search"
                            
                            rel="noopener"
                            title="Suche"
                        >
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Suche</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="Über"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Über</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="http://stackoverflow.com/users"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Stack Overflow"
                        >
                        <i class="sidebar-button-icon fab fa-stack-overflow" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Stack Overflow</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://twitter.com/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Twitter"
                        >
                        <i class="sidebar-button-icon fab fa-twitter" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Twitter</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://facebook.com/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Facebook"
                        >
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://plus.google.com/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Google Plus"
                        >
                        <i class="sidebar-button-icon fab fa-google-plus" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Google Plus</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.linkedin.com/profile/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="LinkedIn"
                        >
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/mailto"
                            
                            rel="noopener"
                            title="E-Mail"
                        >
                        <i class="sidebar-button-icon fa fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">E-Mail</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/atom.xml"
                            
                            rel="noopener"
                            title="RSS"
                        >
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="1"
                 class="
                        hasCoverMetaIn
                        ">
                
    <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a
                            class="link-unstyled"
                            href="/2018/06/06/降维与度量学习/"
                            aria-label=": 降维与度量学习"
                        >
                            降维与度量学习
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-06-06T18:15:00+08:00">
	
		    06 6月 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/机器学习/">机器学习</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>降维在一些图像识别过程也经常被采用的一种分类算法，例如二维数据经过投影变为一维数据，从而更好的表征数据的特征，再进行识别。在前面章节中提到过LDA（线性判别分析）也可以当做一种简单降维处理。</p>
<h2 id="低维嵌入"><a href="#低维嵌入" class="headerlink" title="低维嵌入"></a>低维嵌入</h2><ul>
<li>维数灾难:</li>
<li>缓解维数灾难方法：降维（维数约简），也就是通过某种数学变换将原始高维属性空间转变为一个低维“子空间”，在这个子空间中样本密度大幅提高，距离计算也变得更为容易。 </li>
</ul>
<p>在很多时候，人们观测或收集到的数据样本虽然是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维嵌入。   </p>
<ul>
<li>线性降维方法：基于线性变换来进行降维的方法。</li>
</ul>
<h2 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h2><p> 以二维特征为例，两个特征之间可能存在线性关系的（例如这两个特征分别是运动的时速和秒速度），这样就造成了第二维信息是冗余的。PCA的目标是为了发现这种特征之间的线性关系，检测出这些线性关系，并且去除这线性关系。</p>
<p>定义两个特征之间的协方差：<br>$$σ<em>{jk}=\frac{1}{n-1}\sum</em>{i=1}^n(x_{ij}-\overlinex_j)(x_{ik}-\overlinex_k)$$</p>
<p>多个特征之间的协方差矩阵：<br>$$\sum = \frac{1}{n-1}((X-\overline{x})^T(X-\overline{x}))$$<br>where $\overline{x}=\frac{1}{n}\sum_{k=1}^n x_i$</p>
<p>根据协方差矩阵，求出特征值、特征向量，找到对应特征值较大的k个特征向量组合成为<strong>变换矩阵</strong>，说明这k个特征值在整个特征空间是比较重要的。通过矩阵乘法，我们就把原样本空间压缩了。</p>
<h2 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h2>
                    
                        


                    
                    
                        <p>
                            <a
                                href="/2018/06/06/降维与度量学习/#post-footer"
                                class="postShorten-excerpt_link link"
                                aria-label=""
                            >
                                Kommentieren und teilen
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a
                            class="link-unstyled"
                            href="/2018/05/23/神经网络/"
                            aria-label=": 神经网络"
                        >
                            神经网络
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-05-23T10:41:00+08:00">
	
		    23 5月 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/机器学习/">机器学习</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <h2 id="神经元（neuron）模型"><a href="#神经元（neuron）模型" class="headerlink" title="神经元（neuron）模型"></a>神经元（neuron）模型</h2><p> 神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。<br> 神经网络中最基本的成分是神经元（neuron）模型，即“简单单元”，在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过一个“阈值（threshold）”，那么它就会被激活，即“兴奋” 起来，向其他神经元发送化学物质。</p>
<p><img src="/images/pasted-36.png" alt="upload successful"></p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>理想激活函数是阶跃函数，0 表示抑制神经元而1表示激活神经元<br>阶跃函数具有不连续、不光滑等不好的性质，常用的是Sigmoid函数</p>
<h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4><p><img src="/images/pasted-37.png" alt="upload successful"><br>Sigmoid函数可能在较大范围内变化的输入值挤压到（0,1）输出值范围内，因此有时也称为”挤压函数”<br> 把这样许多个神经元按一定的层次结构连接起来，就得到了神经网络。</p>
<h4 id="ReLu函数"><a href="#ReLu函数" class="headerlink" title="ReLu函数"></a>ReLu函数</h4><h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><h4 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h4><p>$max(0,(y-\widehat{y}))$</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ul>
<li>回归问题：SSE（Sum of Squared Error）均方误差和</li>
<li>分类问题：CE（Cross Entropy）交叉熵</li>
</ul>
<h2 id="感知机（Perceptron）与多层网络"><a href="#感知机（Perceptron）与多层网络" class="headerlink" title="感知机（Perceptron）与多层网络"></a>感知机（Perceptron）与多层网络</h2><p>感知机有两层神经元组成<br><img src="/images/pasted-38.png" alt="upload successful"><br>权重 及阈值θ通过学习获得，阈值θ可看做一个固定输入为-1的哑结点（dummy node）所对应的权重 。这样权重和阈值可以统一学习。对训练样例(x,y)，感知机输出 ，学习规则：<br>$$w_i←w_i+\nabla{w_i}$$<br>$$\nabla{w_i}=η(y-\widehat{y})x_i$$<br>η∈(0,1)称为学习率(learning rate)。<br>感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元。与或非问题都是线性可分（linearly separable）。感知机对线性可分学习过程一定收敛，非线性可分问题w难以稳定下来，不能求合适的解，如下图D。<br><img src="/images/pasted-41.png" alt="upload successful"><br>要解决非线性可分问题，需要考虑使用多层功能神经元<br><img src="/images/pasted-42.png" alt="upload successful"><br><img src="/images/pasted-43.png" alt="upload successful"><br>网络结构中，输入层与输出层之间的神经元层成为隐含层（hidden layer），每层神经元与下一层神经元完全互联，神经元之间不存在同层连接，也不存在跨层连接，称为多层前馈网络结构(multi-layer feedforward nerual networks)</p>
<ul>
<li>多层网络：包含隐层的网络</li>
<li>前馈网络：神经元之间不存在同层连接也不存在跨层连接  </li>
</ul>
<p>隐层和输出层具有激活函数，所以这两层的神经元亦称“功能单元”。多层前馈网络有强大的表示能力。只需一个包含足够多神经元的隐层，多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数。设置隐层神经元数，通常用“试错法”。</p>
<ul>
<li>主要特点：信号是前向传播的，而误差是反向传播的。</li>
<li>主要过程：信号的前向传播，从输入层经过隐含层，最后到达输出层</li>
<li>误差的反向传播，从输出层到隐含层，最后到输入层，依次调节隐含层到输出层的权重和偏置，输入层到隐含层的权重和偏置</li>
</ul>
<h2 id="误差逆传播算法——BP神经网络"><a href="#误差逆传播算法——BP神经网络" class="headerlink" title="误差逆传播算法——BP神经网络"></a>误差逆传播算法——BP神经网络</h2><p> 误差逆传播（error BackPropagation，简称BP）它是迄今为止最成功的神经网络学习算法，现实任务中使用神经网络时，大多在使用BP算法进行训练多层前馈神经网络，还可用于训练例如递归神经网络。</p>
<h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><p>$y=g(x)$，$z=h(y)$，$$\nabla{x}→\nabla{y}→\nabla{z} , \frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}$$<br>$x=g(s)$，$y=h(s)$，$z=k(x,y)$，$$\nabla{s}→\nabla{x},\nabla{y}→\nabla{z} , \frac{dz}{ds}=\frac{dz}{dx}\frac{dx}{ds}+\frac{dz}{dy}\frac{dy}{ds}$$</p>
<h3 id="BP算法过程"><a href="#BP算法过程" class="headerlink" title="BP算法过程"></a>BP算法过程</h3><p>给定训练集：$D=((x_1,y_1),(x_2,y_2)….(x_m,y_m)),x∈\Bbb{R}^d,y∈\Bbb{R}^l,$<br>输入：d维特征向量，（d个属性）；<br>输出：L个输出值（l维实值向量）；<br>隐层：假定使用q个隐层神经元；<br>输出层权重：$w_{ij}$；隐层权重：$v_{ij}$；输出层阈值：$θ_i$；隐层阈值：$γ_i$<br>隐层输入<br><img src="/images/pasted-45.png" alt="upload successful">；<br>输出层输入<br><img src="/images/pasted-46.png" alt="upload successful">；<br>隐层第h个神经元输出：bh；<br>假定功能单元均使用Sigmoid函数 。<br><img src="/images/pasted-47.png" alt="upload successful"><br>对训练<br><img src="/images/pasted-48.png" alt="upload successful">，假定输出为<br><img src="/images/pasted-49.png" alt="upload successful"> ，即<br><img src="/images/pasted-50.png" alt="upload successful">，则网络在<br><img src="/images/pasted-51.png" alt="upload successful">的均方误差为<br><img src="/images/pasted-52.png" alt="upload successful">，未知的参数包括隐层及输出层权值、阈值。<br>BP通过迭代学习，在每一轮采用广义的感知机学习规划对参数进行更新估计：<br><img src="/images/pasted-53.png" alt="upload successful">。BP算法基于梯度下降策略（gradient descent），以目标负梯度方向对参数进行调整。对于误差Ek，给定学习率：η：</p>
<p><img src="/images/pasted-54.png" alt="upload successful"><br>由于<br><img src="/images/pasted-55.png" alt="upload successful">，可得到<br><img src="/images/pasted-56.png" alt="upload successful"> 。<br>Sigmoid函数有以下性质：<br><img src="/images/pasted-57.png" alt="upload successful"> ，所以<br><img src="/images/pasted-58.png" alt="upload successful">：<br><img src="/images/pasted-59.png" alt="upload successful"><br>最终推得：<br><img src="/images/pasted-60.png" alt="upload successful"><br>其他参数的推导式同样的方法：<br><img src="/images/pasted-61.png" alt="upload successful"> 。<br>其中：<br><img src="/images/pasted-62.png" alt="upload successful"><br>学习率<br><img src="/images/pasted-63.png" alt="upload successful">，控制迭代中的更新步长，太大容易震荡，太小则收敛过慢。其中wθ与vγ的学习率不一定相等。</p>
<h3 id="BP算法流程"><a href="#BP算法流程" class="headerlink" title="BP算法流程"></a>BP算法流程</h3><p>算法的工作流程：<br><img src="/images/pasted-64.png" alt="upload successful"></p>
<h3 id="标准BP算法与累计BP算法"><a href="#标准BP算法与累计BP算法" class="headerlink" title="标准BP算法与累计BP算法"></a>标准BP算法与累计BP算法</h3><p>主要目标：最小化训练集D上的累计误差 。前面算法更新规则是基于单个Ek推导的，也称作“标准BP算法”。若使用基于累计误差最小化的更新规则，成为累计误差逆传播算法（accumulated errror backpropagation）。两者都很常用：</p>
<p>| ————- |—–:|<br>|标准BP算法|    1、每次针对单个训练样例更新权值与阈值；2、参数更新频繁，不同样例可以抵消，需要多次迭代|</p>
<p>|累计BP算法|    1、其优化目标是最小化整个训练集上的累计误差；<br>2、读取整个训练集一遍才对参数进行更新，参数更新频率较低|</p>
<p>累计BP算法更新频率低，防止不同样例导致训练出现抵消的现象。在很多任务中，累计误差下降到一定程度后，进一步下降会非常缓慢，这是标准BP算法往往会获得较好的解，尤其当训练集非常大时效果更明显。</p>
<h3 id="缓解过拟合"><a href="#缓解过拟合" class="headerlink" title="缓解过拟合"></a>缓解过拟合</h3><p>主要策略</p>
<ul>
<li>早停early stopping<br>将训练数据分为训练集和验证集。训练集计算梯度和更新，验证估计误差。<br>1、若训练误差连续a轮的变化小于b,则停止训练<br>2、使用验证集：若训练误差降低，验证误差升高，则停止训练。<br>返回具有最小验证误差的链接权重和阈值。</li>
<li>正则化<br>regularization    在误差目标函数中，增加一项描述网络复杂度：例如连接权和阈值的平方和<br>误差目标函数改为： ， 用于对经验误差和网络复杂度进行折中。偏好比较小的连接权和阈值，使网络输出更“光滑”</li>
</ul>
<h2 id="全局最小与局部极小"><a href="#全局最小与局部极小" class="headerlink" title="全局最小与局部极小"></a>全局最小与局部极小</h2><p><img src="/images/pasted-65.png" alt="upload successful"><br>神经网络的训练过程可看作一个参数寻优过程：<br>在参数空间中，寻找一组最优参数使得误差最小<br>特点：存在多个“局部极小”；只有一个“全局最小”<br>常用策略跳出局部极小</p>
<ul>
<li>不同参数进行初始化    </li>
<li>模拟退火（simulated annealing）    以一定概率接收比当前解更差的结果，每部迭代中，接受次优解的概率随时间推移而降低。</li>
<li>随机梯度下降    计算梯度时增加随机因素，即使陷入局部极小也有机会跳出继续搜索。</li>
<li>遗传算法（genetic algorithms）    </li>
</ul>
<h2 id="其他常见神经网络模型"><a href="#其他常见神经网络模型" class="headerlink" title="其他常见神经网络模型"></a>其他常见神经网络模型</h2><h3 id="RBF网络"><a href="#RBF网络" class="headerlink" title="RBF网络"></a>RBF网络</h3><p>RBF（Radial Basis Function，径向基函数）网络在分类任务中除BP之外最常用的一种<br>•    单隐层前馈神经网络<br>•    使用径向基函数作为隐层神经元激活函数ρ： ，定义为样本x到数据中心ci之间欧式距离的单调函数，常用高斯径向基函数。 。ci表示隐层神经元对应的中心、wi表示权重。<br>•    输出层是隐层神经元输出的线性组合<br>训练RBF网络：<br>•    确定神经元中心ci，常用的方式包括随机采样、聚类等<br>•    利用BP算法等确定参数wi和βi。</p>
<h3 id="ART网络"><a href="#ART网络" class="headerlink" title="ART网络"></a>ART网络</h3><p>ART（Adaptive Resonance Theory，自适应谐振理论）竞争学习的代表，是一种常用的无监督学习策略。该策略网络输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活。其他神经元被抑制。包含比较层、识别层、识别阈值和重置模块。</p>
<h3 id="SOM网络"><a href="#SOM网络" class="headerlink" title="SOM网络"></a>SOM网络</h3><p>SOM（Self-Organizing Map，自组织映射）网络是最常用的聚类方法之一：<br>•    竞争型的无监督神经网络<br>•    将高维数据映射到低维空间，并保持输入数据在高维空间的拓扑结构。即将高维空间中相似的样本点映射到网络输出层中邻近神经元<br>•    每个神经元拥有一个权向量<br>•    目标：为每个输出层神经元找到合适的权向量以保持拓扑结构<br>训练<br>•    网络接收输入样本后，将会确定输出层的“获胜”神经元（“胜者通吃”）<br>•    获胜神经元的权向量将向当前输入样本移动</p>
<h3 id="级联相关网络：“构造性”神经网络的代表"><a href="#级联相关网络：“构造性”神经网络的代表" class="headerlink" title="级联相关网络：“构造性”神经网络的代表"></a>级联相关网络：“构造性”神经网络的代表</h3><p>构造性神经网络：将网络结构也当做学习的目标，并在训练过程中找到最符合的网络结构。是结构自适应网络的重要代表。<br>训练<br>•    开始时只有输入层和输出层<br>•    级联（Cascade）：新的隐层节点逐渐加入，从而创建起层级结构<br>•    相关（Correlation）：最大化新节点的输出与网络误差之间的相关性</p>
<h3 id="Elman网络：递归神经网络的代表"><a href="#Elman网络：递归神经网络的代表" class="headerlink" title="Elman网络：递归神经网络的代表"></a>Elman网络：递归神经网络的代表</h3><p>•    网络可以有环形结构，可让使一些神经元的输出反馈回来最为输入<br>•    t 时刻网络的输出状态： 由 t 时刻的输入状态和 t-1时刻的网络状态共同决定<br>Elman网络是最常用的递归神经网络之一<br>•    结构与前馈神经网络很相似，但隐层神经元的输出被反馈回来<br>•    使用推广的BP算法训练</p>
<h3 id="Bolyzmann机：”基于能量的模型”的代表"><a href="#Bolyzmann机：”基于能量的模型”的代表" class="headerlink" title="Bolyzmann机：”基于能量的模型”的代表"></a>Bolyzmann机：”基于能量的模型”的代表</h3><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p>•    卷积神经网络CNN</p>
<p><img src="/images/pasted-66.png" alt="upload successful"><br>•    每个卷积层包含多个特征映射，每个特征映射是一个由多个神经元构成的“平面”，通过一种卷积滤波器提取输入的一种特征<br>•    采样层亦称“汇合层”，其作用是基于局部相关性原理进行亚采样，从而在减少数据量的同事保留有用信息<br>•    连接层就是传统神经网络对隐层与输出层的全连接<br>典型的深度学习模型就是很深层的神经网络</p>

                    
                        


                    
                    
                        <p>
                            <a
                                href="/2018/05/23/神经网络/#post-footer"
                                class="postShorten-excerpt_link link"
                                aria-label=""
                            >
                                Kommentieren und teilen
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a
                            class="link-unstyled"
                            href="/2018/05/16/贝叶斯/"
                            aria-label=": 贝叶斯分类器"
                        >
                            贝叶斯分类器
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-05-16T14:16:00+08:00">
	
		    16 5月 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/机器学习/">机器学习</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul>
<li><strong>条件概率的概念：</strong>A和B是两个事件，事件A发生的条件下，事件B发生的概率，表示为：$P(A|B)=P(AB)/P(B)$。<br>事件独立：$P(AB)=P(A)P(B)$。<br>求和$P(A+B)=P(A)+P(B)-P(AB)$。<br>求差$P(A-B)=P(A)-P(AB)$。<br>乘积：$P(AB)=P(A)P(B|A)=P(B)P(A|B)$。<br>联合分布：$\sum_{A} P(A,B)=P(B)$<br>$P(B)=\sum_{B|A}P(A)$<br>联合分布条件概率：$P(S|W)=\frac {P(S,W)}{P(W)}$<br>$P(A,B|C)=P(A|B,C)P(B|C)$</li>
<li><strong>贝叶斯公式：</strong><br>$P(B│A)=\frac{P(B)P(A|B)}{P(A)}$。<br>由全概率公式，当$B1、B2、B3……∈B，A⊂∑B_i$时，$P(B_i│A)=P(B_i)P(A|B_i)/(∑P(B_j)P(A|B_j))$。<ul>
<li>$P(B_i)$先验概率</li>
<li>$P(B_i│A)$为后验概率。</li>
</ul>
</li>
</ul>
<p>贝叶斯分类器是利用概率的知识完成数据的分类任务，在机器学习中使用贝叶斯决策论实施决策的基本方法也是在概率的框架下进行的，它是考虑如何基于这些概率和误判损失来选择最优的类别标记。</p>
<h2 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h2><p>假设有N种可能的类别标记$y=(c_1,c_2….c_N)$，$λ_{ij}$是将一个真实标记为$c_j$的样本误分类为$c_i$所产生的损失。基于后验概率$P(c_i|x)$可获得奖样本x分类为$c_i$所产生的期望损失，即在样本x上的“条件风险”。<br>$$R(c_i|x=\sum_{j=1}^Nλ_{ij}P(c_j|x))$$<br>机器学习的过程就是要寻找一个判定准则：$h:x–&gt;y$以最小化总体风险。<br>为最小化总体风险，只需要在每个样本上选择哪个能使条件分析最小的类别标记。<br>$$h^<em>(x)=arg min R(c|x)$$<br>此时，$h^</em>$称为贝叶斯最优分类器，与之对应的总体风险$R(h^<em>)$称为贝叶斯风险。则$1-R(h^</em>)$反映了分类器所能达到的最好性能。 </p>
<ul>
<li>生成式模型： 先对联合概率分布P(x,c)建模，然后由此获得P(c|x)，如：贝叶斯分类器。</li>
<li>判别式模型： 给定x，通过直接建模P(c|x)，预测c。如：决策树、BP神经网络、支持向量机</li>
</ul>
<h2 id="极大拟然估计"><a href="#极大拟然估计" class="headerlink" title="极大拟然估计"></a>极大拟然估计</h2><p>概率模型的训练过程就是参数估计过程，采用极大拟然估计就是试图在所有的可能的取值中，找到一个能使数据出现的“可能性”的最大值。  </p>
<ul>
<li>频率主义学派（Frequentist）：参数虽然未知，但是客观存在固定值，所以可以通过优化似然函数等准则确定参数值。极大似然估计（Maximum Likelihood Estimation，MLE）是根据数据采样来估计概率分布的。  </li>
<li>贝叶斯学派（Bayesian）：参数是未观察到的随机变量，其本身也可有分布，因此，可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。</li>
</ul>
<h2 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h2><p>朴素贝叶斯分类器（Naïve Bayes classifier）：基于贝叶斯公式来估计后验概率 的主要困难在于求类条件概率 是所有属性上的联合概率，难以从有限的训练样本直接估计得到。为了解决这个问题，提出朴素贝叶斯分类器 它采用了“属性条件独立假设”对已知类别，假设所有属性相互独立，换言之，假设每个属性独立地对分类结果发生影响。<br><img src="/images/pasted-79.png" alt="upload successful"><br><img src="/images/pasted-80.png" alt="upload successful"></p>
<h2 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h2><p>为了降低贝叶斯公式中估计后验概率$P(c|x)$的困难，提出使用朴素贝叶斯分类器采用属性条件独立性假设，然而在现实任务中这个假设很难成立，因此就提出半朴素贝叶斯分类器（Sem-naïve Bayes classifiers），它的基本思想是适当考虑一部分属性间的相互依赖信息，从而既不需要完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。<br>独依赖估计（One-Dependent Estimator，ODE）每个属性在类别之外最多仅依赖于一个其他属性。最直接的做法，假设所有属性都依赖于同一属性，称为为超父（super-parent ODE）。TAN（Tree Augmented naïve Bayes）使用最大生成树算法，将属性间依赖关系生成最大带权生成树，保留了强相关属性之间的依赖性。AODE（Average的 One-Dependent Estimator）基于集成学习机制，尝试将每个属性作为超父来构建SPODE，最终返回具有足够支撑的SPODE集成作为最终结果。<br><img src="/images/pasted-81.png" alt="upload successful"></p>
<h2 id="贝叶斯网"><a href="#贝叶斯网" class="headerlink" title="贝叶斯网"></a>贝叶斯网</h2><p>贝叶斯网是借助有向无环图（DAG）来刻画属性之间的依赖关系，并使用条件概率表（CPT）来描述属性的联合概率分布<br>结构<br>学习<br>推断<br><img src="/images/pasted-82.png" alt="upload successful"></p>
<h2 id="经典贝叶斯问题"><a href="#经典贝叶斯问题" class="headerlink" title="经典贝叶斯问题"></a>经典贝叶斯问题</h2><p>A女士怀疑自己得了某种肝炎，希望在医院做一次检测。医生告诉A女士，她所属的人群得此种肝炎的概率仅有千分之一。但A女士不放心，还是坚持做了测试。然而很不幸，测试结果为阳性。现在已知测试仪器的正确率为95%，那么A女士确实得了肝炎而非误诊的概率为多少？</p>

                    
                        


                    
                    
                        <p>
                            <a
                                href="/2018/05/16/贝叶斯/#post-footer"
                                class="postShorten-excerpt_link link"
                                aria-label=""
                            >
                                Kommentieren und teilen
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a
                            class="link-unstyled"
                            href="/2018/05/10/SVM/"
                            aria-label=": SVM"
                        >
                            SVM
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-05-10T15:50:00+08:00">
	
		    10 5月 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/机器学习/">机器学习</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><h3 id="间隔与支持间隔"><a href="#间隔与支持间隔" class="headerlink" title="间隔与支持间隔"></a>间隔与支持间隔</h3><p>分类学习的最基本想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。<br>能将训练样本划分开的平面可能有很多个，选择位于两类训练样本正中间的划分超平面，原因是这个超平面的分类结果最鲁棒，泛化能力最强。<br>在样本空间中，划分超平面可通过以下线性方程来描述<br>$$ω^Tx+b=0$$<br>样本空间中任意点 x 到超平面 (w,b) 的距离可以写成<br>$$r=\frac{|w^Tx+b|}{||w||}$$<br>假设超平面能够正确分类样本，则可以通过对 ω 缩放可以使得下式成立<br>\begin{cases}<br>|w^Tx+b|&gt;=1,y_i=+1<br>|w^Tx+b|&lt;=-1,y_i=-1<br>\end{cases}<br>距离超平面最近的几个样本点使得上式等号成立，称作“支持向量”。两个异类支持向量到超平面的距离之和称为“间隔”，$γ=\frac{2}{||w||}$。所有位于最大边界上的点称作支持向量。撑起了边界的宽度，其中$||w||$是向量的范数（norm）：<br>$$\if W = {w_1,w_2,….w_n}then\sqrt{W·W}=\sqrt{w_1^2,w_2^2,….w_n^2}$$<br><img src="/images/pasted-27.png" alt="upload successful"><br>距离是x和x’在w上的投影：<br><img src="/images/pasted-28.png" alt="upload successful"><br>欲最大化间隔，等价于最小化$||w||^2$ , 这就是支持向量机的基本型。<br>$$\min{w,b} \frac{1}{2} ||w||^2$$<br>$$s.t. y_i(w^Tx_i+b)≥1，i=1,2,…,m$$<br><img src="/images/pasted-29.png" alt="upload successful"></p>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>上述问题是一个凸二次规划问题，能直接用现成的优化计算包求解。但是通过拉格朗日乘子法变换到对偶变量的优化问题之后，可以找到一种更加有效的方法来进行求解。<br>原问题的拉格朗日函数为<br>$$L(w,b,α)=\frac{1}{2}||w||^2+\sum{i=1}^m α_i(1-y_i(w^Tx_i+b))$$<br>求偏导为零可以得到<br>$$w=\sum{i=1}^m α_iy_ix_i$$<br>$$0=\sum{i=1}^mα_iy_i$$<br>对偶问题，将最大最小互换：<br>$$\min{w,b}\max{α}L(w,b,α)-&gt;\max{α}\min{w,b}L(w,b,α)$$<br>$$\max{α} \sum{i=1}{m}α_i-\frac{1}{2}\sum{i=1}^m\sum{j=1}^mα_iα_jy_iy_jx_i^Tx_j{}$$<br>$$s.t. \sum{i=1}^mα_iy_i=0,α_i≥0，i=1,2,…,m$$<br>于是可以得到模型为<br>$$f(x)=w^Tx+b=\sum{i=1}^mα_iy_ix_i^Tx+b$$<br>上述过程需要满足KKT(Karush-Kuhn-Tucker)条件，即<br>\begin{cases}<br>α_i≥0<br>y_if(x_i)-1≥0<br>α_i(y_if(x_i)-1)=0<br>\end{cases}<br>对任意训练样本 $(x_i,y_i)$ ，总有 $α_i=0$ 或 $y_if(x_i)=1$ 。因此训练完成后，大部分的样本都不需要保留，最终模型仅与支持向量有关。<br>如果用二次规划算法求解对偶问题，则问题的规模正比于训练样本数，这会在实际任务中造成很大开销，为此提出SMO(Sequential Minimal Optimization)算法。</p>
<h4 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h4><p>SMO将比较大的问题拆解为比较小的问题，多个变量不好求，先求两个。<br>步骤：不断执行以下两个步骤直到收敛<br>1、选取一对需要更新的变量 αi 和 αj<br>2、固定 αi 和 αj 以外的参数，求解对偶问题更新后的 αi 和 αj<br>只要选取的 αi 和 αj 中有一个不满足KKT条件， 目标函数就会在迭代后减小。KKT条件违背的程度越大，变量更新后可能导致的目标函数值减幅越大。<br>使选取的两变量所对应样本之间的间隔最大（两个变量有很大的差别，对它们进行更新会带给目标函数值更大的变化）。</p>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>原始样本空间线性不可分：将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。如果原始空间是有限维，那么一定存在一个高维特征空间使样本可分。<br>模型变成：$ω^Tϕ(x)+b=0 $<br>$$\min{w,b}\frac{1}{2}||w||^2$$<br>$$s.t. y_i(w^Tϕ(x_i)+b)≥1，i=1,2,…m$$<br>对偶问题为：<br>$$\max{α} \sum{i=1}{m}α_i-\frac{1}{2}\sum{i=1}^m\sum{j=1}^mα_iα_jy_iy_jx_i^Tx_j{}$$<br>$$s.t. \sum{i=1}^mα_iy_i=0,α_i≥0，i=1,2,…,m$$<br>由于特征空间维数可能很高，直接计算 $ϕ(x_i)^Tϕ(x_j)$通常是困难的。设想函数$k(x_i,x_j)=ϕ(x_i)Tϕ(x_j)$,$x_ixi$与$x_jxj$在特征空间的内积<strong>等于</strong>它们在原始样本空间中通过核函数计算的结果。<br>核函数选择成为支持向量机的最大变数，若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。比如图像分类，一般使用高斯径向基核函数，因为需要超平面要非常的平滑。<br>常用核函数：<br><img src="/images/pasted-30.png" alt="upload successful"></p>
<h2 id="软间隔与正则化"><a href="#软间隔与正则化" class="headerlink" title="软间隔与正则化"></a>软间隔与正则化</h2><p>现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分，即便线性可分，也很难判定这个结果不是由于过拟合造成的。<br>缓解这个问题的一个方法是允许支持向量机在一些样本上出错，引入“软间隔”概念。允许某些样本不满足约束 $y_i(ω^Txi+b)≥1 $<br>优化目标可写成<br>$$\min{w,b}\frac{1}{2}||w||^2+C\sum{i=1}^mξ_i$$<br>其中 l0/1 是“0/1损失函数”<br> <img src="/images/pasted-11.png" alt="upload successful"><br>l0/1 非凸、非连续、数学性质不好，使得上式难以求解，因此人们用其他一些函数来代替它，称为“代替函数”。<br>常用的代替函数：<br><img src="/images/pasted-21.png" alt="upload successful"><br>常用的软间隔支持向量机：<br><img src="/images/pasted-31.png" alt="upload successful"><br>与硬间隔支持向量机相似，软间隔支持向量机也是一个二次规划问题，可以通过拉格朗日乘子法得到拉格朗日函数：<br><img src="/images/pasted-32.png" alt="upload successful"><br>求偏导为零可以得到<br><img src="/images/pasted-33.png" alt="upload successful"><br>对偶问题为<br><img src="/images/pasted-34.png" alt="upload successful"><br>上述过程需要满足KKT(Karush-Kuhn-Tucker)条件，即<br><img src="/images/pasted-35.png" alt="upload successful"><br>实际上支持向量机和对率回归的优化目标相近，通常情况下他们的性能相当。对率回归的优势主要对于其输出具有自然的概率意义，即在给出预测标记的同时也给了概率，而支持向量机的输出不具有概率意义，欲得到概率需要进行特殊处理；此外，对率回归能够直接用于多分类任务，支持向量机为此需要进行推广。另一方面，可以看出hinge损失函数有一块平摊的零区域，这使得支持向量机的解具有稀疏性，而对率损失是光滑的而单调递减函数，不能导出类似支持向量的概念。因此对率回归的解依赖于更多的训练样本，其预测开销大。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X=np.r_[np.random.randn(<span class="number">20</span>,<span class="number">2</span>)-[<span class="number">2</span>,<span class="number">2</span>],np.random.randn(<span class="number">20</span>,<span class="number">2</span>)+[<span class="number">2</span>,<span class="number">2</span>]]</span><br><span class="line">Y=[<span class="number">0</span>]*<span class="number">20</span>+[<span class="number">1</span>]*<span class="number">20</span></span><br><span class="line">clf=svm.SVC(kernel=<span class="string">'linear'</span>)</span><br><span class="line">clf.fit(X,Y)</span><br><span class="line">w=clf.coef_[<span class="number">0</span>]</span><br><span class="line">a=-w[<span class="number">0</span>]/w[<span class="number">1</span>]</span><br><span class="line">xx=np.linspace(<span class="number">-5</span>,<span class="number">5</span>)</span><br><span class="line">yy=a*xx-(clf.intercept_[<span class="number">0</span>])/w[<span class="number">1</span>]</span><br><span class="line">b=clf.support_vectors_[<span class="number">0</span>]</span><br><span class="line">yy_down=a*xx+b[<span class="number">1</span>]-a*b[<span class="number">0</span>]</span><br><span class="line">b=clf.support_vectors_[<span class="number">-1</span>]</span><br><span class="line">yy_up=a*xx+b[<span class="number">1</span>]-a*b[<span class="number">0</span>]</span><br><span class="line">pl.plot(xx,yy,<span class="string">'k-'</span>)</span><br><span class="line">pl.plot(xx,yy_down,<span class="string">'k--'</span>)</span><br><span class="line">pl.plot(xx,yy_up,<span class="string">'k--'</span>)</span><br><span class="line">pl.scatter(clf.support_vectors_[:,<span class="number">0</span>],clf.support_vectors_[:,<span class="number">1</span>])</span><br><span class="line">pl.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=Y,cmap=pl.cm.Paired)</span><br><span class="line">pl.show()</span><br></pre></td></tr></table></figure></p>

                    
                        


                    
                    
                        <p>
                            <a
                                href="/2018/05/10/SVM/#post-footer"
                                class="postShorten-excerpt_link link"
                                aria-label=""
                            >
                                Kommentieren und teilen
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a
                            class="link-unstyled"
                            href="/2018/04/24/聚类算法/"
                            aria-label=": 聚类算法"
                        >
                            聚类算法
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-04-24T19:41:00+08:00">
	
		    24 4月 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/机器学习/">机器学习</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>聚类可以说是一种无监督的学习，也就是说在训练样本中对应的标记信息是没有的，目标是通过对无标记训练样本的学习来揭示数据内在性质和规律，为进一步的数据分析提供基础。</p>
<h3 id="聚类的任务"><a href="#聚类的任务" class="headerlink" title="聚类的任务"></a>聚类的任务</h3><p>聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”。</p>
<h3 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h3><p>簇内相似度高，簇间相似度低。 </p>
<ul>
<li>外部指标：是将聚类结果与某个“参考模型”进行比较 </li>
<li>内部指标：直接考察聚类结果而不利用任何参考模型。</li>
</ul>
<h3 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h3><p>距离度量满足的基本性质：非负性、同一性、对称性、直递性 </p>
<ul>
<li>闵可夫斯基距离：$distmk=(n∑u=1|xiu−xju|p)1p$<ul>
<li>如果p=2时，则表示欧氏距离 </li>
<li>如果p=1时，则表示曼哈顿距离</li>
</ul>
</li>
<li>有序属性： </li>
<li>无序属性：闵可夫斯基可以用于无序属性。对于无序属性可以采用VDM</li>
</ul>
<h2 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h2><p>原型聚类亦称“基于原型的聚类”，常用的原型聚类算法如下 </p>
<ul>
<li>K均值聚类(Kmeans)</li>
<li>学习向量量化</li>
<li><a href="http://www.cnblogs.com/mmziscoming/p/5750849.html" target="_blank" rel="noopener">高斯混合聚类(GMM)</a></li>
</ul>
<p><a href="http://www.cnblogs.com/kemaswill/archive/2013/01/26/2877434.html" target="_blank" rel="noopener">K值的确定</a>：<br><a href="https://www.zhihu.com/question/29208148/answer/141482198" target="_blank" rel="noopener">知乎的回答</a></p>
<ul>
<li>数据的先验知识，或者数据进行简单分析能得到</li>
<li>基于变化的算法：即定义一个函数，随着K的改变，认为在正确的K时会产生极值。如Gap Statistic（Estimating the number of clusters in a data set via the gap statistic, Tibshirani, Walther, and Hastie 2001），Jump Statistic （finding the number of clusters in a data set, Sugar and James 2003）</li>
<li>基于结构的算法：即比较类内距离、类间距离以确定K。这个也是最常用的办法，如使用平均轮廓系数，越趋近1聚类效果越好；如计算类内距离/类间距离，值越小越好；等。</li>
<li>基于一致性矩阵的算法：即认为在正确的K时，不同次聚类的结果会更加相似，以此确定K。基于层次聚类：即基于合并或分裂的思想，在一定情况下停止从而获得K。</li>
<li>基于采样的算法：即对样本采样，分别做聚类；根据这些结果的相似性确定K。如，将样本分为训练与测试样本；对训练样本训练分类器，用于预测测试样本类别，并与聚类的类别比较。</li>
</ul>
<h2 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h2><p>基于密度的聚类算法主要的目标是寻找被低密度区域分离的高密度区域。与基于距离的聚类算法不同的是，基于距离的聚类算法的聚类结果是球状的簇，而基于密度的聚类算法可以发现任意形状的聚类，这对于带有噪音点的数据起着重要的作用。 </p>
<h2 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h2><p>层次聚类也叫连通聚类方法，有两个基本方法：自顶而下和自底而上。自顶而将所有样本看做是同一簇，然后进行分裂。自底而上将初所有样本看做不同的簇，然后进行凝聚。这种聚类的中心思想是：离观测点较近的点相比离观测点较远的点更可能是一类。<br>过程如下：</p>
<ol>
<li>把每个样本归为一类，计算两个类之间的距离；</li>
<li>寻找各类之间最近的两个类归为一类；</li>
<li>重新计算新生成的这个类与旧类之间的相似度；</li>
<li>重复2到3，直到所有样本归为一类停止。</li>
</ol>
<p>实际上聚类过程就是建立了一棵树：<br><img src="/images/pasted-76.png" alt="upload successful"><br>可以在中间过程设置阈值，当两个类距离大于阈值，则迭代终止。</p>
<p>第三步相似度的度量有很多方法，例如：</p>
<ul>
<li>两个类之间距离最近的两个样本的距离</li>
<li>两个类之间距离最远的两个样本的距离</li>
<li>两个类的样本平均距离</li>
<li>两个类的样本中位数的距离</li>
</ul>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="ART网络"><a href="#ART网络" class="headerlink" title="ART网络"></a>ART网络</h3><p>ART（Adaptive Resonance Theory，自适应谐振理论）竞争学习的代表，是一种常用的无监督学习策略。该策略网络输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活。其他神经元被抑制。包含比较层、识别层、识别阈值和重置模块。</p>
<h3 id="SOM网络"><a href="#SOM网络" class="headerlink" title="SOM网络"></a>SOM网络</h3><p><img src="/images/pasted-77.png" alt="upload successful"><br>SOM（Self-Organizing Map，自组织映射）网络是最常用的聚类方法之一： </p>
<ul>
<li>竞争型的无监督神经网络</li>
<li>将高维数据映射到低维空间，并保持输入数据在高维空间的拓扑结构。即将高维空间中相似的样本点映射到网络输出层中邻近神经元</li>
<li>每个神经元拥有一个权向量</li>
<li>目标：为每个输出层神经元找到合适的权向量以保持拓扑结构</li>
</ul>
<p>训练</p>
<ul>
<li>网络接收输入样本后，将会确定输出层的“获胜”神经元（“胜者通吃”）</li>
<li>获胜神经元的权向量将向当前输入样本移动</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span><span class="params">(x,k,maxIt)</span>:</span></span><br><span class="line">    numPoints,numDim=x.shape</span><br><span class="line">    dataSet=np.zeros((numPoints,numDim+<span class="number">1</span>))</span><br><span class="line">    dataSet[:,:<span class="number">-1</span>]=x</span><br><span class="line">    centroids=dataSet[np.random.randint(numPoints,size=k),:]</span><br><span class="line"><span class="comment">#     centroids=dataSet[0:2,:]</span></span><br><span class="line">    centroids[:,<span class="number">-1</span>]=range(<span class="number">1</span>,k+<span class="number">1</span>)</span><br><span class="line">    iterations=<span class="number">0</span></span><br><span class="line">    oldCentroids=<span class="keyword">None</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> shouldstop(oldCentroids,centroids,iterations,maxIt):</span><br><span class="line">        print(iterations,dataSet,centroids)</span><br><span class="line">        oldCentroids=np.copy(centroids)</span><br><span class="line">        iterations+=<span class="number">1</span></span><br><span class="line">        updateLabels(dataSet,centroids)</span><br><span class="line">        centroids=getCentroids(dataSet,k)</span><br><span class="line">    <span class="keyword">return</span> dataSet</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shouldstop</span><span class="params">(oldCentroids,centroids,iterations,maxIt)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> iterations&gt;maxIt:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">return</span> np.array_equal(oldCentroids,centroids)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateLabels</span><span class="params">(dataSet,centroids)</span>:</span></span><br><span class="line">    numPoints,numDim=dataSet.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,numPoints):</span><br><span class="line">        dataSet[i,<span class="number">-1</span>]=getLabelFromClosestCentroid(dataSet[i,:<span class="number">-1</span>],centroids)        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLabelFromClosestCentroid</span><span class="params">(dataSetRow,centroids)</span>:</span></span><br><span class="line">    label=centroids[<span class="number">0</span>,<span class="number">-1</span>]</span><br><span class="line">    minDist=np.linalg.norm(dataSetRow-centroids[<span class="number">0</span>,:<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,centroids.shape[<span class="number">0</span>]):</span><br><span class="line">        dist=np.linalg.norm(dataSetRow-centroids[i,:<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">if</span> dist&lt;minDist:</span><br><span class="line">            minDist=dist</span><br><span class="line">            label=centroids[i,<span class="number">-1</span>]</span><br><span class="line">    print(minDist)</span><br><span class="line">    <span class="keyword">return</span> label        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCentroids</span><span class="params">(dataSet,k)</span>:</span></span><br><span class="line">    result=np.zeros((k,dataSet.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,k+<span class="number">1</span>):</span><br><span class="line">        oneCluster=dataSet[dataSet[:,<span class="number">-1</span>]==i,:<span class="number">-1</span>]</span><br><span class="line">        result[i<span class="number">-1</span>,:<span class="number">-1</span>]=np.mean(oneCluster,axis=<span class="number">0</span>)</span><br><span class="line">        result[i<span class="number">-1</span>,<span class="number">-1</span>]=i</span><br><span class="line">    <span class="keyword">return</span> result        </span><br><span class="line">x1=np.array([<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">x2=np.array([<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">x3=np.array([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">x4=np.array([<span class="number">5</span>,<span class="number">4</span>])</span><br><span class="line">testX=np.vstack((x1,x2,x3,x4))</span><br><span class="line"></span><br><span class="line">result=kmeans(testX,<span class="number">2</span>,<span class="number">10</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>层次聚类的实现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image,ImageDraw</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cluster_node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,vec,left=None,right=None,distance=<span class="number">0.0</span>,id=None,count=<span class="number">1</span>)</span>:</span></span><br><span class="line">        self.vec=vec</span><br><span class="line">        self.left=left</span><br><span class="line">        self.right=right</span><br><span class="line">        self.distance=distance</span><br><span class="line">        self.id=id</span><br><span class="line">        self.count=count</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2dist</span><span class="params">(v1,v2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.linalg.norm(v1-v2,ord=<span class="number">2</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hcluster</span><span class="params">(features,distance=L2dist)</span>:</span></span><br><span class="line">    distances=&#123;&#125;</span><br><span class="line">    currentclustid=<span class="number">-1</span></span><br><span class="line">    clust=[Cluster_node(array(features[i]),id=i) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(features))]</span><br><span class="line">    <span class="keyword">while</span> len(clust)&gt;<span class="number">1</span>:</span><br><span class="line">        lowestpair=(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        closest=distance(clust[<span class="number">0</span>].vec,clust[<span class="number">1</span>].vec)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(clust)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,len(clust)):</span><br><span class="line">                <span class="keyword">if</span> (clust[i].id,clust[j].id) <span class="keyword">not</span> <span class="keyword">in</span> distances:</span><br><span class="line">                    distances[(clust[i].id,clust[j].id)]=distance(clust[i].vec,clust[j].vec)</span><br><span class="line">                d=distances[(clust[i].id,clust[j].id)]</span><br><span class="line">                <span class="keyword">if</span> d&lt;closest:</span><br><span class="line">                    closest=d</span><br><span class="line">                    lowestpair=(i,j)</span><br><span class="line">        mergevec=[(clust[lowestpair[<span class="number">0</span>]].vec[i]+clust[lowestpair[<span class="number">1</span>]].vec[i])/<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(clust[<span class="number">0</span>].vec))]</span><br><span class="line">        newcluster=Cluster_node(array(mergevec),left=clust[lowestpair[<span class="number">0</span>]],right=clust[lowestpair[<span class="number">1</span>]],distance=closest,id=currentclustid)</span><br><span class="line">        currentclustid-=<span class="number">1</span></span><br><span class="line">        <span class="keyword">del</span> clust[lowestpair[<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">del</span> clust[lowestpair[<span class="number">0</span>]]</span><br><span class="line">        clust.append(newcluster)</span><br><span class="line">    <span class="keyword">return</span> clust[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_clusters</span><span class="params">(clust,dist)</span>:</span></span><br><span class="line">    clusters=&#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> clust.distance&lt;dist:</span><br><span class="line">        <span class="keyword">return</span> [clust]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        cl=[]</span><br><span class="line">        cr=[]</span><br><span class="line">        <span class="keyword">if</span> clust.left!=<span class="keyword">None</span>:</span><br><span class="line">            c1=extract_clusters(clust.left,dist=dist)</span><br><span class="line">        <span class="keyword">if</span> clust.right!=<span class="keyword">None</span>:</span><br><span class="line">            cr=extract_clusters(clust.right,dist=dist)</span><br><span class="line">        <span class="keyword">return</span> cl+cr</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cluster_elements</span><span class="params">(clust)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> clust.id&gt;=<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> [clust.id]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        cl=[]</span><br><span class="line">        cr=[]</span><br><span class="line">        <span class="keyword">if</span> clust.left!=<span class="keyword">None</span>:</span><br><span class="line">            c1=get_cluster_elements(clust.left)</span><br><span class="line">        <span class="keyword">if</span> clust.right!=<span class="keyword">None</span>:</span><br><span class="line">            cr=get_cluster_elements(clust.right)</span><br><span class="line">        <span class="keyword">return</span> cl+cr</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printclust</span><span class="params">(clust,labels=None,n=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        print(<span class="string">' '</span>,end=<span class="string">''</span>)</span><br><span class="line">    <span class="keyword">if</span> clust.id&lt;<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'-'</span>,end=<span class="string">''</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> labels==<span class="keyword">None</span>:</span><br><span class="line">            print(clust.id,end=<span class="string">''</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(labels[clust.id],end=<span class="string">''</span>)</span><br><span class="line">    <span class="keyword">if</span> clust.left!=<span class="keyword">None</span>:</span><br><span class="line">        printclust(clust.left,labels=labels,n=n+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> clust.right!=<span class="keyword">None</span>:</span><br><span class="line">        printclust(clust.right,labels=labels,n=n+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getheight</span><span class="params">(clust)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> clust.left==<span class="keyword">None</span> <span class="keyword">and</span> clust.right==<span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> getheight(clust.left)+getheight(clust.right)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getdepth</span><span class="params">(clust)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> clust.left==<span class="keyword">None</span> <span class="keyword">and</span> clust.right==<span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> max(getheight(clust.left),getheight(clust.right))+clust.distance</span><br></pre></td></tr></table></figure></p>
<p>下面是几个城市的GDP等信息，根据这些信息，写一个SOM网络，使之对下面城市进行聚类。并且，将结果画在一个二维平面上。</p>
<p>//表1中，X。为人均GDP(元)；X2为工业总产值(亿元)；X。为社会消费品零售总额(亿元)；x。为批发零售贸易总额(亿元)；x。为地区货运总量(万吨)，表1中数据来自2002年城市统计年鉴。</p>
<p>//城市 X1 X2 X3 Xa X5<br>北京 27527 2738.30 1494.83 3055.63 30500<br>青岛 29682 1212.02 182.80 598.06 29068<br>天津 22073 2663.56 782.33 1465.65 28151<br>烟台 21017 298.73 92.71 227.39 8178<br>石家庄 25584 467.42 156.02 763.46 12415<br>郑州 17330 261.80 215.63 402.98 7373<br>唐山 19387 338.67 95.73 199.69 14522<br>武汉 17882 1020.84 685.82 1452 16244<br>太原 13919 304.13 141.94 155.22 15170<br>长沙 26327 241.76 269.93 369.83 7550<br>呼和浩特 13738 82.23 69.27 108.12 2415<br>衡阳 12386 61.53 63.95 72.65 3004<br>沈阳 21736 729.04 590.26 1752.4 15156<br>广州 42828 2446.97 1166.10 3214.19 24500<br>大连 34659 1003.56 431.83 728.08 19736<br>深圳 152099 3079.63 609.26 801.06 5167<br>长春 24799 900.26 309.75 173.99 10346<br>油头 19414 192.93 112.96 280.84 1443<br>哈尔滨 20737 402.73 360.38 762.94 8814<br>湛江 15290 228.45 99.08 149.16 5524<br>上海 40788 6935.57 1531.89 3921.2 49499<br>南宁 17715 109.39 142.08 264.32 3371<br>南京 26697 1579.21 401.20 1253.73 14120<br>柳州 17598 256.76 68.93 159.44 3397<br>徐州 19727 295.73 108.17 187.39 7124<br>海口 24782 100.13 81.03 142.54 2018<br>连云港 17869 112.18 47.94 134.89 4096<br>成都 22956 412.23 400.56 754.07 23724<br>杭州 31784 1615.63 373.28 1788.29 15841<br>重庆 9778 870.82 389.60 823.72 29470<br>宁波 46471 751.58 167.70 529.68 11182<br>贵阳 13176 207.95 108.93 285.27 4885<br>温州 29781 381.93 233.44 272.84 6292<br>昆明 24554 303.78 227.44 428.64 12084<br>合肥 19770 330.14 140.14 328.98 2903<br>西安 16002 449.14 323.37 558.27 7728<br>福州 33570 379.51 209.72 613.24 7280<br>兰州 16629 354.30 163.97 374.9 5401<br>厦门 42039 803.29 186.55 620.47 2547<br>西宁 7261 38.00 48.95 91.14 1837<br>南昌 19923 238.82 14.09 348.21 3246<br>银川 12779 77.74 41.22 53.16 1573<br>济南 25642 616.97 323.08 462.39 13057<br>乌鲁木齐 19793 251.19 129.05 277.8 9283</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">result = pd.read_csv(<span class="string">'./data/diqu.csv'</span>,sep=<span class="string">'\s+'</span>,encoding=<span class="string">'gbk'</span>)</span><br><span class="line"><span class="comment"># 将result归一化</span></span><br><span class="line"><span class="comment"># 标准化操作：from sklearn.preprocessing import StanderdScaler</span></span><br><span class="line"><span class="comment"># result_std=StandardScaler().fit_transform(reslut)</span></span><br><span class="line">result_norm = (result.iloc[:,<span class="number">1</span>:] - result.iloc[:,<span class="number">1</span>:].min()) / (result.iloc[:,<span class="number">1</span>:].max() - result.iloc[:,<span class="number">1</span>:].min())</span><br><span class="line">result_norm.head()</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">kmeans_model=KMeans(n_clusters=<span class="number">4</span>,random_state=<span class="number">1</span>)</span><br><span class="line">senator_distances=kmeans_model.fit_transform(result_norm.iloc[:,<span class="number">1</span>:])</span><br><span class="line">labels=kmeans_model.labels_</span><br><span class="line">print(pd.crosstab(labels,result[<span class="string">'城市'</span>]))</span><br><span class="line"><span class="comment"># democratic_outliers=votes[(labels==1)&amp;(result['party']!='D')]</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter(x=senator_distances[:,<span class="number">0</span>],y=senator_distances[:,<span class="number">1</span>],c=labels)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/pasted-78.png" alt="upload successful"></p>

                    
                        


                    
                    
                        <p>
                            <a
                                href="/2018/04/24/聚类算法/#post-footer"
                                class="postShorten-excerpt_link link"
                                aria-label=""
                            >
                                Kommentieren und teilen
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a
                            class="link-unstyled"
                            href="/2018/04/19/KNN/"
                            aria-label=": KNN"
                        >
                            KNN
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-04-19T18:14:00+08:00">
	
		    19 4月 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/机器学习/">机器学习</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <h2 id="K邻近学习"><a href="#K邻近学习" class="headerlink" title="K邻近学习"></a>K邻近学习</h2><p>k近邻学习是一种监督学习算法，lazy-learning算法。训练复杂度为0。在给定的训练样本集中，基于某种距离度量，找出与训练集最靠近的k个训练样本，然后基于这k个邻居信息来进行预测。<br><strong>投票法：</strong>通常在分类任务中使用，判别方法是选择这k个样本中出现最多的雷冰标记作为预测结果。（所以k值取值一般为奇数，以便可以得到有偏向的投票结果）<br><strong>平均法：</strong>通常在回归任务中使用，判别方法是将这k个样本的实值输出标记的平均值最为预测结果。<br><strong>加权平均或加权投票：</strong>根据距离远近来决定权重，距离越近，权重越大。<br><img src="http://p8cigu7up.bkt.clouddn.com/1526725719271s4cfc85r.png?imageslim" alt="paste image"><br>增加权重后，就可以避免由于密集的样本造成的影响。</p>
<h3 id="KNN算法主要过程"><a href="#KNN算法主要过程" class="headerlink" title="KNN算法主要过程"></a>KNN算法主要过程</h3><ol>
<li>计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li>
<li>对上面所有的距离值进行排序；</li>
<li>选前k个最小距离的样本；</li>
<li>根据这k个样本的标签进行投票，得到最后的分类类别；</li>
</ol>
<p>如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。<br>近邻算法具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。</p>
<h3 id="距离衡量方法"><a href="#距离衡量方法" class="headerlink" title="距离衡量方法"></a>距离衡量方法</h3><h4 id="Euclidean-Distance"><a href="#Euclidean-Distance" class="headerlink" title="Euclidean Distance"></a>Euclidean Distance</h4><p>$D=\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$</p>
<h4 id="余弦（cos）"><a href="#余弦（cos）" class="headerlink" title="余弦（cos）"></a>余弦（cos）</h4><h4 id="相关度-correlation"><a href="#相关度-correlation" class="headerlink" title="相关度(correlation)"></a>相关度(correlation)</h4><p>$$cos(θ)=\frac{A·B}{||A||×||B||}$$</p>
<h4 id="曼哈顿距离-Manhattan-distance"><a href="#曼哈顿距离-Manhattan-distance" class="headerlink" title="曼哈顿距离(Manhattan distance)"></a>曼哈顿距离(Manhattan distance)</h4><p>走过的街区数。</p>
<h3 id="KNN算法的优缺点"><a href="#KNN算法的优缺点" class="headerlink" title="KNN算法的优缺点"></a>KNN算法的优缺点</h3><p><strong>优点：</strong>  </p>
<ol>
<li>思想简单，理论成熟，既可以用来做分类也可以用来做回归；</li>
<li>可用于非线性分类；</li>
<li>训练时间复杂度为O(n)；</li>
<li>准确度高，对数据没有假设，对outlier不敏感；</li>
</ol>
<p><strong>缺点：</strong>  </p>
<ol>
<li>计算量大；</li>
<li>样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；</li>
<li>需要大量的内存；</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors,datasets</span><br><span class="line">knn=neighbors.KNeighborsClassifier()</span><br><span class="line">iris=datasets.load_iris()</span><br><span class="line">knn.fit(iris.data,iris.target)</span><br><span class="line">predictedLabel=knn.predict([[<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>]])</span><br><span class="line">print(predictedLabel)</span><br></pre></td></tr></table></figure>
                    
                        


                    
                    
                        <p>
                            <a
                                href="/2018/04/19/KNN/#post-footer"
                                class="postShorten-excerpt_link link"
                                aria-label=""
                            >
                                Kommentieren und teilen
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a
                            class="link-unstyled"
                            href="/2018/04/15/人工智能基本概念/"
                            aria-label=": 人工智能——决策树算法"
                        >
                            人工智能——决策树算法
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-04-15T14:19:00+08:00">
	
		    15 4月 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/人工智能/">人工智能</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树（decision tree）是什么?要想解决这个问题，首先要弄明白的就是计算机中的树是什么。树，我们在计算机中很常见了，有二叉树，哈夫曼树等等，总结一下共同点的时候就是，对一个当前节点而言，下一个个节点有多个节点可以选择的结构。简单的说就是有分叉的结构就是树（可能这样说也不严谨）。而决策树就是利用了这种分叉来判断的树。<br>决策树以树状结构表示数据分类结果。包含</p>
<ul>
<li>一个根节点</li>
<li>若干个非叶子节点（决策点、测试条件）</li>
<li>若干个叶子结点（分类后所得的分类标记）</li>
<li>分支（测试结果）<br><img src="/images/pasted-4.png" alt="upload successful"></li>
</ul>
<h3 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h3><h4 id="熵的概念"><a href="#熵的概念" class="headerlink" title="熵的概念"></a>熵的概念</h4><p>几个重要的概念：</p>
<ul>
<li>由$P(X,Y)=P(X)*P(Y)$事件X和Y相互独立的特性，可以得知：$$\log_{}(XY)=\log_{}(X)+\log_{}(Y)$$  </li>
<li>$H(X)$：事件X发生的不确定性。$P(X)$越小，$H(X)$越小，$P(X)$越大，$H(X)$越大。<br>信息量的大小和它的不确定性有直接的关系，要搞清楚意见非常不确定的事情，就需要大量的信息。<br>香农布朗就用比特（bit）来衡量信息量的多少<br>$$ H(X)=-\sum_{i} p_i \log_{2}p_i$$<br><strong>熵的计算公式</strong><br>熵：表征一个物体内部的混乱程度。<br>$$Entropy=-\sum_{i=1}^n p_i \ln_{p_i}$$<br>Gini系数：$$Gini(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^2$$<br>决策树划分节点的规则：节点熵迅速降低。熵降低的速度越快越好。将熵值最大的当做根节点，分类后，再继续判断下一层的熵值。<h4 id="KL散度-相对熵"><a href="#KL散度-相对熵" class="headerlink" title="KL散度-相对熵"></a>KL散度-相对熵</h4>Kullback-Leibler(KL)divergence<br>$$D_{KL}(P||Q)=\Bbb{E}_{x~P}[\log \frac{P(x)}{Q(x)}]$$<br>KL散度性质：</li>
<li>非负性：KL散度为0当且仅当P和Q同分布。</li>
<li>非对称性：$D_{KL}(P||Q)\neq D_{KL}(Q||P)=$</li>
</ul>
<h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h4><p>cross-entropy<br>$$H(P,Q)=H(P)+D_{KL}(P||Q)$$</p>
<h3 id="归纳算法"><a href="#归纳算法" class="headerlink" title="归纳算法"></a>归纳算法</h3><h4 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h4><p>度量方法：信息获取量（information Gain）纯度差，也称为信息增益。例如通过A作为节点分类获取了多少信息 ：<br>$$Gain(D,a)=Ent(D)-\sum_{v=1}^V \frac{|D^v|}{D}Ent(D^v)$$<br>Ent(D)代表不纯度</p>
<p><img src="/images/pasted-5.png" alt="upload successful"></p>
<p>例如按照年龄区分时，得到信息获取量：</p>
<p><img src="/images/pasted-12.png" alt="upload successful"></p>
<p>然后按照收入、是否是学生、信用率求得对应的值，用信息量最大的参数当做当前节点划分的依据。不断重复这个步骤，直到满足停止条件停止。</p>
<p>这个就是ID3算法的大致步骤。</p>
<h4 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h4><p>度量方法：Gain Ration<br>现在有一个极端的例子，给每个样本进行编号，然后按照编号弄一个决策树可不可以呢？答案当然是不可以。因为每个编号都是一类，这样无疑是最准确的，但是有一个问题就是泛化能力远远不够啊。这样做了是毫无意义的。由于ID3对于信息中类似索引的数值特别的敏感，这个属性会将决策树过分的划分出很多分支，造成模型效果性能较差。<br>C4.5是ID3算法的拓展，引入了增益率，比上自身的熵值。 因此我们用增益率来防止这个事情的发生。<br>$$Gain-ratio(D,a)=\frac{Gain(D,a)}{IV(a)}其中IV(a)=-\sum_{v=1}^V \frac{|D^v|}{|D|}\log_{2}\frac{|D^v|}{|D|}$$<br>属性A的可能取值数目越多，则IV(A)的值通常会较大。增益率会对取值数目较少的属性有所偏好，C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p>
<h4 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h4><p>Classification and Regression Trees<br>度量方法：Gini系数<br>$$Gain-index(D,a)=\sum_{v=1}^V \frac{|D^v|}{|D|}Gini(D^v)$$</p>
<h3 id="评价函数"><a href="#评价函数" class="headerlink" title="评价函数"></a>评价函数</h3><p>评价函数是评估所有叶子节点的纯度很大，所以定义评价函数C(T)。<br>$$C(T)=\sum_{t∈leaf}N_t·H(t)$$<br>$N_t$表示叶子结点的个数（相当于权重值），乘上熵值。评价决策树的优劣，C(T)的值越小越好。</p>
<h3 id="停止条件"><a href="#停止条件" class="headerlink" title="停止条件"></a>停止条件</h3><p>决策树的构建过程是一个递归的过程，所以需要确定停止条件，否则过程将不会结束。</p>
<ul>
<li>一般一种最直观的方式是当每个子节点只有一种类型的记录时停止，但是这样往往会使得树的节点过多，导致过拟合问题（Overfitting）。</li>
<li>另一种可行的方法是当前节点中的记录数低于一个最小的阀值，那么就停止分割，将max(P(i))对应的分类作为当前叶节点的分类。</li>
</ul>
<h3 id="优化方案"><a href="#优化方案" class="headerlink" title="优化方案"></a>优化方案</h3><p>策树高度过高，会导致模型的过拟合。这样如果存在错误样本，会导致模型性能下降。一般通过预剪枝、后剪枝优化决策树。</p>
<ul>
<li><strong>预剪枝：</strong>首先设定决策树最大深度，在决预剪枝在构建过程中，如果查过了最大深度则提前停止。</li>
<li><strong>后剪枝：</strong>当构建完成决策树后，根据<strong>评价函数</strong>$$C_{\alpha}(T)=C(T)+\alpha|T_{leaf}|$$不光看$C(T)$损失函数，还要看叶子结点的个数越多，也要进行剪枝。$\alpha$是可以指定的。<br>参数α≥0控制两者之间的影响，较大的α促使选择较简单的模型，较小的α促使选择较复杂的模型。剪枝就是当α确定时，选择损失函数最小的模型，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好，损失函数正好表示了对两者的平衡。</li>
</ul>
<h3 id="连续与缺失值"><a href="#连续与缺失值" class="headerlink" title="连续与缺失值"></a>连续与缺失值</h3><p><strong>连续值处理</strong><br>由于连续属性的可取值数目不再有限，因此不能直接根据连续属性的可取值来对接点进行划分，此时，连续属性离散化技术可派上用场，最简单的策略是采用二分法对连续属性进行处理。给定样本集D和连续属性A，假定A在D上出现了n个不同的取值，将这些值从小到大进行排序，记为${a_1,a_2…a_n}$，对相邻属性取值来说，在区间$(a_i,a_{i+1})$中取任意值所产生的划分结果相同，因此对连续属性A，可以考察包含n-1个元素的候选划分点集合。</p>
<pre><code>需要注意，与离散属性不同，划分节点时，若当前节点划分属性为连续属性，该属性还可以作为其后代节点的划分属性。
</code></pre><p><strong>缺失值处理</strong><br>有时候会遇到不完整样本，某些属性值缺失，如果简单放弃不完整样本，显然是对数据的浪费。使用则需要解决两个问题：(1) 如何在属性值缺失的情况下进行划分属性选择？(2) 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？<br>对问题一：令$\tilde{D}$表示在a上没有缺失值的样本子集。假定a有V个可能取值。 $\tilde{D}^v$表示a取$a^v$的样本子集。假定每个样本x赋予一个权重$w_x$，并定义:<br>$$\rho=\frac{\sum_{x∈\tilde{D}}w_x}{\sum_{x∈D}w_x}$$</p>
<p>$$\tilde{p_k}=\frac{\sum_{x∈\tilde{D}_k}w_x}{\sum_{x∈\tilde{D}}w_x}(1≤k≤|\upsilon|)$$</p>
<p>$$\tilde{r_v}=\frac{\sum_{x∈\tilde{D}^v}w_x}{\sum_{x∈\tilde{D}}w_x} (1≤v≤V)$$<br>a、ρ表示无缺失样本值所占比例。 表示第k类所占比例，$\tilde{r_v}$表示无缺失样本在属性a上取值$a^v$所占比例。固将增益计算式推广为：<br>$$Gain(D,a)=\rho \times Gain(\tilde{D},a)=\rho*\Bigl(Ent(\tilde{D})-\sum_{v=1}^V \tilde{r_v}Ent(\tilde{D^v})\Bigr)$$</p>
<p>对问题二：简单说就是让没有取值的x划入所有子节点，属性值调整为$\tilde{r_v}·w_x$ </p>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>决策树由于容易产生过拟合，出现了随机森林，减小了过拟合现象。<br>随机森林是通过多个决策树进行组合生成的。</p>
<h2 id="python实现"><a href="#python实现" class="headerlink" title="python实现"></a>python实现</h2><p>数据是这样的，根据年龄、收入、是否是学生、信用数据构建决策树：<br><img src="/images/pasted-15.png" alt="upload successful"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing,tree</span><br><span class="line"><span class="keyword">from</span> sklearn.externals.six <span class="keyword">import</span> StringIO</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd_data = pd.read_csv(<span class="string">'data/tree.csv'</span>)</span><br><span class="line">labelList=[]</span><br><span class="line">featureList=[]</span><br><span class="line">f= open(<span class="string">'data/tree.csv'</span>,<span class="string">'r'</span>,encoding=<span class="string">'utf8'</span>)</span><br><span class="line">reader = csv.reader(f)</span><br><span class="line">labelList=[]</span><br><span class="line">featureList=[]</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">    labelList.append(row[len(row)<span class="number">-1</span>])</span><br><span class="line">    rowDict=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(row)<span class="number">-1</span>):</span><br><span class="line">        rowDict[pd_data.columns[i]]=row[i]</span><br><span class="line">    featureList.append(rowDict)</span><br><span class="line">featureList.pop(<span class="number">0</span>)</span><br><span class="line">labelList.pop(<span class="number">0</span>)</span><br><span class="line">vec=DictVectorizer()</span><br><span class="line">dummyX=vec.fit_transform(featureList).toarray()</span><br><span class="line">lb=preprocessing.LabelBinarizer()</span><br><span class="line">dummyY=lb.fit_transform(labelList)</span><br><span class="line">clf=tree.DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)  <span class="comment"># 默认为CART</span></span><br><span class="line">clf=clf.fit(dummyX,dummyY)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data/tree.dot'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f=tree.export_graphviz(clf,feature_names=vec.get_feature_names(),out_file=f)</span><br></pre></td></tr></table></figure>
<p>windows中，可以安装<a href="http://www.graphviz.org" target="_blank" rel="noopener">Graphviz</a>将dot文件转为pdf，可视化决策树。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dot -Tpdf tree.dot -o tree.pdf</span><br></pre></td></tr></table></figure>
<p><img src="/images/pasted-14.png" alt="upload successful"></p>
<p>样本数据进行交叉验证：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">(training_inputs,</span><br><span class="line"> testing_inputs,</span><br><span class="line"> training_classes,</span><br><span class="line"> testing_classes) = train_test_split(dummyX, dummyY, train_size=<span class="number">0.75</span>, random_state=<span class="number">1</span>)</span><br><span class="line"> clf=tree.DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)  <span class="comment"># 默认为CART，设置为ID3</span></span><br><span class="line">clf=clf.fit(training_inputs,training_classes)</span><br><span class="line">clf.score(testing_inputs, testing_classes)</span><br></pre></td></tr></table></figure></p>
<p>可以看出，这个决策树的得分是0.5，是个很低的预测结果，原因可能是数据太少了，交叉验证后供决策树提供训练的数据不足。可以调整参数，尝试使用Gini系数得分也一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor </span><br><span class="line">rf=RandomForestRegressor()  </span><br><span class="line">rf.fit(training_inputs,training_classes)</span><br><span class="line">rf.score(testing_inputs, testing_classes)</span><br></pre></td></tr></table></figure>
<p>随机森林由于随机性，每次得出的结果不一定相同。</p>

                    
                        


                    
                    
                        <p>
                            <a
                                href="/2018/04/15/人工智能基本概念/#post-footer"
                                class="postShorten-excerpt_link link"
                                aria-label=""
                            >
                                Kommentieren und teilen
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a
                            class="link-unstyled"
                            href="/2018/04/11/人工智能数学基础-1/"
                            aria-label=": 数学基础"
                        >
                            数学基础
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-04-11T21:46:00+08:00">
	
		    11 4月 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/机器学习/">机器学习</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <h2 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h2><p><strong>夹逼定理：</strong>$g(x)&lt;=f(x)&lt;=h(x)$成立，且$\lim_{x\to0}g(x)=A$<br>$\lim_{x\to0}h(x)=A$,则$\lim_{x\to0}f(x)=A$<br><strong>极限的定义：</strong>$\lim_{x\to0}sinx/x=1$,<br><img src="http://p8cigu7up.bkt.clouddn.com/1526046740175opawbglw.png?imageslim" alt="paste image"><br><strong>分步积分</strong><br><img src="http://p8cigu7up.bkt.clouddn.com/1526046782042q7n1b2rc.png?imageslim" alt="paste image"><br>应用：例如：求$f(x)最小值$<br><img src="http://p8cigu7up.bkt.clouddn.com/1526046811476uagtkvuy.png?imageslim" alt="paste image"></p>
<p><img src="http://p8cigu7up.bkt.clouddn.com/15260468682169yoeit5n.png?imageslim" alt="paste image"></p>
<h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p><strong>方向导数：</strong><img src="http://p8cigu7up.bkt.clouddn.com/1526046905297mbojtto7.png?imageslim" alt="paste image"></p>
<p>φ为x轴到方向L的转角</p>
<p><strong>梯度：</strong><img src="http://p8cigu7up.bkt.clouddn.com/1526047051671cujiauam.png?imageslim" alt="paste image"><br>梯度方向是函数在该点变化最快的方向。</p>
<h2 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h2><p><strong>凸函数：</strong><img src="http://p8cigu7up.bkt.clouddn.com/1526047197413vo87brcr.png?imageslim" alt="paste image"><br><img src="http://p8cigu7up.bkt.clouddn.com/1526047219531agv1nvou.png?imageslim" alt="paste image"><br><strong>一阶可微</strong><br>凸函数的切线，一定位于函数图像的下方。<br><img src="http://p8cigu7up.bkt.clouddn.com/1526047331546ayoqpcus.png?imageslim" alt="paste image"><br><strong>二阶可微</strong><br>函数满足二阶可微，则为凸函数的条件表示为<img src="http://p8cigu7up.bkt.clouddn.com/1526047415799hmcryljw.png?imageslim" alt="paste image"><br><img src="http://p8cigu7up.bkt.clouddn.com/1526047424737iw8g1sbd.png?imageslim" alt="paste image"><br>常见的凸函数<br><img src="http://p8cigu7up.bkt.clouddn.com/1526047472799y6c896ka.png?imageslim" alt="paste image"><br><strong>Jensen不等式</strong><br><img src="http://p8cigu7up.bkt.clouddn.com/1526047508849usc7kybd.png?imageslim" alt="paste image"><br>表示凸函数，如果做一个推广：<br><img src="http://p8cigu7up.bkt.clouddn.com/1526047561126kgkoyamk.png?imageslim" alt="paste image"><br>表示的是x1~xk的线性加权，小于函数值的线性加权。<br>如果θ表示的概率分布的话，得到下面的结论：<br><img src="http://p8cigu7up.bkt.clouddn.com/1526047656173g0pa24l9.png?imageslim" alt="paste image"><br>期望的函数&lt;=函数的期望</p>
<p>Jesen不等式非常重要，几乎是所有不等式的基础。</p>
<h3 id="共轭函数"><a href="#共轭函数" class="headerlink" title="共轭函数"></a>共轭函数</h3><p>f是$\Bbb{R}^n–&gt;\Bbb{R}$是一个函数，那么f的共轭函数<br>$$f^*(y)=\sup {x∈dom f}(x^Tx-f(x))$$<br>以x为斜率，f(x)为截距。<br>共轭函数是描述在x点处，函数的斜率和截距之间的全体。<br><img src="http://p8cigu7up.bkt.clouddn.com/1526730911799r042hrac.png?imageslim" alt="paste image"><br><strong>共轭函数的性质</strong>  </p>
<ul>
<li>共轭函数$f^*$是一个凸函数</li>
<li>如果g是f的凸闭，那么$g^<em>=f^</em>$</li>
<li>如果f是一个凸函数，那么$f^{**}=f$</li>
</ul>
<p><img src="/images/pasted-13.png" alt="upload successful"></p>
<h3 id="拉格朗日对偶函数"><a href="#拉格朗日对偶函数" class="headerlink" title="拉格朗日对偶函数"></a>拉格朗日对偶函数</h3><p><img src="/images/pasted-16.png" alt="upload successful"><br><img src="/images/pasted-17.png" alt="upload successful"><br><img src="/images/pasted-18.png" alt="upload successful"><br><img src="/images/pasted-19.png" alt="upload successful"><br><img src="/images/pasted-20.png" alt="upload successful"></p>
<h2 id="Taylor展开式"><a href="#Taylor展开式" class="headerlink" title="Taylor展开式"></a>Taylor展开式</h2><p>$$f(x)=f(x_0)+f’(x_0)(x-x_0)+\frac{f’’(x_0)}{2!}(x-x_0)^2+…+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x)$$<br>在$x=0$对$f(x)$进行Taylor展开就是Maclaurin公式<br>$$f(x)=f(0)+f’(0)x+\frac{f’’(0)}{2!}x^2+…+\frac{f^{(n)}(x)}{n!}x^n+o(x^{n})$$</p>
<h3 id="计算函数值"><a href="#计算函数值" class="headerlink" title="计算函数值"></a>计算函数值</h3><p>Taylor公式的应用首先可以计算初等函数值，一般在远点展开<br>$$\sin x=x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\frac{x^9}{9!}-…+(-1)^{m-1}\frac{x^{2m-1}}{(2m-1)!}+R_{2m}$$<br>$$e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+…+\frac{x^n}{n!}+R_n$$<br>通过泰勒展开，可以对函数值最近似求解。例如下题<br><img src="http://p8cigu7up.bkt.clouddn.com/1526176929364zt7rzxq6.png?imageslim" alt="paste image"><br>k是整数，$2^k$比较好求，$e^r$就可以通过Taylor展开近似的求解</p>
<h3 id="Gini系数公式"><a href="#Gini系数公式" class="headerlink" title="Gini系数公式"></a>Gini系数公式</h3><p>Gini系数、熵、分类误差率有下图显示的关系：<br><img src="http://p8cigu7up.bkt.clouddn.com/15261766665534b21yx3j.png?imageslim" alt="paste image"><br>熵的定义为：$H(x)=-\sum_{k=1}^K p_k·\ln p_k$<br>Gini系数利用了$f(x)=-\ln x$在$x=1$处的一阶展开，忽略高阶无穷小，近似得到$f(x)\approx 1-x$，熵就可以近似求解为：$H(x)\approx \sum_{k=1}^K p_k·(1-p_k)$</p>
<h3 id="平方根公式"><a href="#平方根公式" class="headerlink" title="平方根公式"></a>平方根公式</h3><p>Taylor公式的另一个应用，可以求解平方根，例如一个数a的平方根的值，$a=x^2$，令$f(x)=x^2-a$，即$f(x)=0$<br>$$ f(x)=f(x_0)+f’(x_0)(x-x_0)+o(x)$$<br>在任意点$x_0$做Taylor展开可得：$f(x)=0\approx f(x_0)+f’(x_0)(x-x_0)$—-&gt;$x-x_0=\frac{f(x_0)}{f’(x_0)}$—-&gt;$x=x_0+\frac{f(x_0)}{f’(x_0)}$  </p>
<p>由于$f(x_0)=x_0^2-a$，$f’(x_0)=2x_0$，所以可得$x=\frac{1}{2}(x_0+\frac{a}{x_0})$，$f(x_0)≠0$，给定任意一个x0，求解x1，然后将x1再带入公式求解x2，直到xn-xn+1足够小的时候，作为平方根的解，一般5至6次就可以得到很好的解了。</p>
<h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p>如果要估计θ，而且已经建立了目标函数J(θ)，例如最小二乘法建立的目标函数（损失函数）：$$J(\theta)=\frac{1}{2}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$$，想计算一个$θ^*$，可以使得$J(θ)$最小。计算方法：随机或者先验的给定$θ$初值，然后沿着负梯度方向迭代，更新$θ$使得$J(θ)$减小，$\theta_j=\theta_j-\alpha·\frac{\partial J(\theta)}{\partial \theta}$，$\alpha$是学习率。  </p>
<h3 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h3><p><strong>牛顿法</strong><br>通过Taylor展开可不可以做一些处理呢？若$f(x)$二阶导连续，那么$f(x)$在$x_k$处做Taylor展开：<br>$$\phi(x)=f(x_k)+f’(x_k)(x-x_k)+\frac{1}{2}f’’(x_k)(x-x_k)^2+R_2(x)$$<br>$$\approx f(x_k)+f’(x_k)(x-x_k)+\frac{1}{2}f’’(x_k)(x-x_k)^2=0$$<br>$$x_{k+1}=x_k-\frac{f’(x_k)}{f’’(x_k)}$$<br>这个公式就是牛顿法，本质上是用二次函数做近似，求解二次函数的极值点，而梯度下降法师用一次函数做近似，学习率控制步长。所以二阶收敛性，可以使得目标函数（线性回归、Logistic回归）的问题中收敛速度快。<br>但是牛顿法要求初始点尽量靠近极小点，否则有可能不收敛。原因：</p>
<ol>
<li>Hessian矩阵奇异，也就是二阶导数为零，牛顿方向不存在；</li>
<li>Hessian非正定，牛顿方向可能是反方向。<br><img src="http://p8cigu7up.bkt.clouddn.com/1526182698594tz1oorv4.png?imageslim" alt="paste image"><br><strong>拟牛顿法</strong><br>拟牛顿法的思路，通过近似矩阵代替Hessian矩阵，只要满足矩阵正定，容易求逆。<br>DFP和BFGS是两种拟牛顿的方法。<h2 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h2>概率$P(x)$<br>分布函数$\Phi(x)=P(x&lt;=x_0)$<h3 id="概率公式"><a href="#概率公式" class="headerlink" title="概率公式"></a>概率公式</h3><strong>条件概率：</strong>$P(A|B)=\frac{P(AB)}{P(B)}$<br><strong>条件概率的链式法则：</strong>$P(a,b,c)=P(a|b,c)P(b,c)=P(a|b,c)P(b|c)P(c)$<br><strong>全概率公式：</strong>$P(A)=\sum_{i} P(A|B_i)P(B_i)$<br><strong>贝叶斯（Bayes）公式：</strong>$P(B_i|A)=\frac{P(A|B_i)P(B_i)}{\sum_{j}P(A|B_j)P(B_j)}$<br>对于求解<a href="http://www.cnblogs.com/AndyJee/p/4714781.html" target="_blank" rel="noopener">碰面问题</a>的概率，可以通过画出x和y所有可能的区域，再求出碰面的范围，再计算面积得到。<br><img src="http://p8cigu7up.bkt.clouddn.com/1526188781608ucrz546h.png?imageslim" alt="paste image"><br>如果使用积分，需要考虑两种情况，一个是x&lt;=3,x可以等1个小时，另一个是x&gt;3，x只能等4-x小时。计算稍微复杂一些。<br><strong>事件独立：</strong>$P(AB)=P(A)P(B)$事件A⊥B正交。<br><strong>条件独立：</strong>A⊥B|Z，$P(A,B|Z)=P(A|Z)P(B|Z)$<br><strong>期望：</strong>1、离散型：$E(x)=\sum_{i}x_ip_i$；2、连续型：$E(x)=\int_\infty^{-\infty} xf(x)$(概率加权的平均值)<br>期望的性质：$E(kX)=kE(X)$，$E(X+Y)=E(X)+E(Y)$，若X和Y相互独立$E(XY)+E(X)E(Y)$<br><strong>方差：</strong>$Var(X)=E{[X-E(X)]^2}=E(X^2)-E^2(X)$<br>方差的性质：$Var(c)=0$，$Var(X+c)=Var(X)$，$Var(kX)=k^2Var(X)$，若X和Y相互独立$Var(X+Y)=Var(X)+Var(Y)$<br><strong>协方差：</strong>$Cov(X,Y)=E(XY)-E(X)E(Y)$，当X和Y独立时，$Cov(X,Y)=0$。协方差表示两个随机变量变化趋势的度量，大于零表示趋势相同，小于零趋势相反，等于零表示不相关。<br>协方差的性质：若$Var(X)=\sigma_1^2$；$Var(Y)=\sigma_2^2$，则$|Cov(X,Y)|&lt;=\sigma_1\sigma_2$。当X和Y之间有线性关系时，等号成立。例如：X=aY+b。<br><strong>相关系数：</strong>$\rho_{XY}=\frac{Cov(X,Y)}{Var(X)Var(Y)}$<br>相关系数的性质：$|\rho_|&lt;=1$<br><strong>协方差矩阵：</strong>当有多个随机向量，每两个随机向量间都可以得到一个协方差，从而组成了协方差矩阵，协方差矩阵时对称阵。$$<br>\begin{bmatrix}<br>c_{11} &amp; c_{12} &amp; \cdots &amp; c_{1n} \\<br>c_{21} &amp; c_{22} &amp; \cdots &amp; c_{2n} \\<br>\vdots &amp; \vdots &amp; \ddots  &amp; \vdots \\<br>c_{n1} &amp; c_{n2} &amp; \cdots &amp; c_{nn} \\<br>\end{bmatrix}<br>$$<h3 id="矩"><a href="#矩" class="headerlink" title="矩"></a>矩</h3>k阶原点矩$E(X^k)$<br>k阶中心距$E { [X-E(X)]^k } $<br>通过矩的定义可以看出，期望是1阶原点矩，方差是2阶中心距。三阶的统计量偏度，四阶统计量是峰度。<br>偏的方向尾巴更长。<br>左偏（负偏）|右偏（正偏）<br><img src="http://p8cigu7up.bkt.clouddn.com/1526195691472ao96bwj9.png?imageslim" alt="paste image"><br>峰度度量的是峰的陡峭程度。通常定义四阶中心矩除以方差的平方减3。减3为了让正态分布的峰度为0，超值峰度为正，称为尖峰态，负为低峰态。它是反应陡峭程度，但需要跟其他结合分析。<br><strong>契比雪夫不等式：</strong>随机变量X的期望$\mu$和方差$\sigma^2$，对于任意正数$\epsilion$$P({|X-\mu|&gt;=\epsilon})&lt;=\frac{\sigma^2}{\epsilon^2}$<br><strong>大数定理：</strong>随机变量X1~Xn相互独立，并且具有相同的期望$\mu$和方差$\sigma^2$，$Y_n=\frac{1}{n}\sum_{i=1}^nX_i$，对于任意正整数$\epsilon$<br>$\lim_{n\to\infty}P({|Y_n-\mu|&lt;\epsilon})=1$<br>大数定理为实际应用中频率来估计概率提供了理论依据。正态分布的参数估计，朴素贝叶斯做垃圾邮件分类，隐马尔科夫模型有监督参数学习等。<br><strong>伯努利定理：</strong>是大数定理最早的形式，表争了事件A发生的频率收敛于事件A发生的概率P。</li>
</ol>
<h2 id="统计学"><a href="#统计学" class="headerlink" title="统计学"></a>统计学</h2><h3 id="统计量"><a href="#统计量" class="headerlink" title="统计量"></a>统计量</h3><p><strong>样本：</strong>$X_1，X_1，…X_n$<br><strong>样本均值：</strong>$\overline{X}=/frac{1}{n}/sum_{i=1}^n(X_i-\overline{X})^2$<br><strong>样本方差：</strong>$S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X})^2$（n-1保证无偏）<br><strong>k阶样本原点矩：</strong>$A_k=\frac{1}{n}\sum_{i=1}^nX_i^k$<br><strong>k阶样本中心矩：</strong>$M_k=\frac{1}{n}\sum_{i=1}^n(X_i-\overline{X})^k$</p>
<h3 id="矩估计"><a href="#矩估计" class="headerlink" title="矩估计"></a>矩估计</h3><p>有一些独立同分布的样本，待求均值$\mu$和方差$\sigma^2$，有原点矩表达式是这样的：<br>$$\begin{cases}<br>E(X)=\mu \\<br>E(X^2)=Var(X)+[E(X)]^2=\sigma^2+\mu^2 \\<br>\end{cases}<br>$$<br>根据样本可以求得原点矩：<br>$$\begin{cases}<br>A_1=\frac{1}{n} \sum_{i=1}^n X_i \\<br>A_2=\frac{1}{n} \sum_{i=1}^n X_i^2 \\<br>\end{cases}<br>$$<br>这样就可以求出矩估计的均值$\widehat{\mu}$和方差$\widehat{\sigma}^2$。求得最终的表达式为：<br>$$\begin{cases}<br>\widehat{\mu}=\overline{X} \\<br>\widehat{\sigma}^2=\frac{1}{n} \sum_{i=1}^n (X_i-\overline{X})^2 \\<br>\end{cases}<br>$$</p>
<h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>以后完善</p>

                    
                        


                    
                    
                        <p>
                            <a
                                href="/2018/04/11/人工智能数学基础-1/#post-footer"
                                class="postShorten-excerpt_link link"
                                aria-label=""
                            >
                                Kommentieren und teilen
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
        
          <li class="pagination-next">
            <a
                class="btn btn--default btn--small"
                href="/archives/page/2/"
                aria-label="ÄLTERE BEITRÄGE"
            >
              <span>ÄLTERE BEITRÄGE</span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
        <li class="pagination-number">Seite 1 von 4</li>
    </ul>
</div>

</section>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2020 hero576. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <h4 id="about-card-name">hero576</h4>
        
            <div id="about-card-bio"><p>author.bio</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>author.job</p>

            </div>
        
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/jquery.js"></script>
<script src="/assets/js/jquery.fancybox.js"></script>
<script src="/assets/js/thumbs.js"></script>
<script src="/assets/js/tranquilpeak.js"></script>
<!--SCRIPTS END-->





    </body>
</html>
