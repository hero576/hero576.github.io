
<!DOCTYPE html>
<html lang="zh-CN">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="hero576的博客">
    <title>Archiv: 2018/5 - hero576的博客</title>
    <meta name="author" content="hero576">
    
        <meta name="keywords" content="py通红,">
    
    
    
    <script type="application/ld+json">{}</script>
    <meta name="description" content="并无特长">
<meta name="keywords" content="py通红">
<meta property="og:type" content="blog">
<meta property="og:title" content="hero576的博客">
<meta property="og:url" content="http://guoming576.cn/archives/2018/05/index.html">
<meta property="og:site_name" content="hero576的博客">
<meta property="og:description" content="并无特长">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="hero576的博客">
<meta name="twitter:description" content="并无特长">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/all.css">
    <link rel="stylesheet" href="/assets/css/jquery.fancybox.css">
    <link rel="stylesheet" href="/assets/css/thumbs.css">
    <link rel="stylesheet" href="/assets/css/tranquilpeak.css">
    <!--STYLES END-->
    

    

    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="1">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/ "
            aria-label=""
        >
            hero576的博客
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Öffne den Link: /#about"
            >
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="1">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Kategorien"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Kategorien</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archiv"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archiv</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link open-algolia-search"
                            href="#search"
                            
                            rel="noopener"
                            title="Suche"
                        >
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Suche</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="#about"
                            
                            rel="noopener"
                            title="Über"
                        >
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Über</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://github.com/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="GitHub"
                        >
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="http://stackoverflow.com/users"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Stack Overflow"
                        >
                        <i class="sidebar-button-icon fab fa-stack-overflow" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Stack Overflow</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://twitter.com/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Twitter"
                        >
                        <i class="sidebar-button-icon fab fa-twitter" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Twitter</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://facebook.com/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Facebook"
                        >
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://plus.google.com/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Google Plus"
                        >
                        <i class="sidebar-button-icon fab fa-google-plus" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Google Plus</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.linkedin.com/profile/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="LinkedIn"
                        >
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/mailto"
                            
                            rel="noopener"
                            title="E-Mail"
                        >
                        <i class="sidebar-button-icon fa fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">E-Mail</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/atom.xml"
                            
                            rel="noopener"
                            title="RSS"
                        >
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="1"
                 class="
                        hasCoverMetaIn
                        ">
                
    <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a
                            class="link-unstyled"
                            href="/2018/05/23/神经网络/"
                            aria-label=": 神经网络"
                        >
                            神经网络
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-05-23T10:41:00+08:00">
	
		    23 5月 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/机器学习/">机器学习</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <h2 id="神经元（neuron）模型"><a href="#神经元（neuron）模型" class="headerlink" title="神经元（neuron）模型"></a>神经元（neuron）模型</h2><p> 神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。<br> 神经网络中最基本的成分是神经元（neuron）模型，即“简单单元”，在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过一个“阈值（threshold）”，那么它就会被激活，即“兴奋” 起来，向其他神经元发送化学物质。</p>
<p><img src="/images/pasted-36.png" alt="upload successful"></p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>理想激活函数是阶跃函数，0 表示抑制神经元而1表示激活神经元<br>阶跃函数具有不连续、不光滑等不好的性质，常用的是Sigmoid函数</p>
<h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4><p><img src="/images/pasted-37.png" alt="upload successful"><br>Sigmoid函数可能在较大范围内变化的输入值挤压到（0,1）输出值范围内，因此有时也称为”挤压函数”<br> 把这样许多个神经元按一定的层次结构连接起来，就得到了神经网络。</p>
<h4 id="ReLu函数"><a href="#ReLu函数" class="headerlink" title="ReLu函数"></a>ReLu函数</h4><h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><h4 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h4><p>$max(0,(y-\widehat{y}))$</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ul>
<li>回归问题：SSE（Sum of Squared Error）均方误差和</li>
<li>分类问题：CE（Cross Entropy）交叉熵</li>
</ul>
<h2 id="感知机（Perceptron）与多层网络"><a href="#感知机（Perceptron）与多层网络" class="headerlink" title="感知机（Perceptron）与多层网络"></a>感知机（Perceptron）与多层网络</h2><p>感知机有两层神经元组成<br><img src="/images/pasted-38.png" alt="upload successful"><br>权重 及阈值θ通过学习获得，阈值θ可看做一个固定输入为-1的哑结点（dummy node）所对应的权重 。这样权重和阈值可以统一学习。对训练样例(x,y)，感知机输出 ，学习规则：<br>$$w_i←w_i+\nabla{w_i}$$<br>$$\nabla{w_i}=η(y-\widehat{y})x_i$$<br>η∈(0,1)称为学习率(learning rate)。<br>感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元。与或非问题都是线性可分（linearly separable）。感知机对线性可分学习过程一定收敛，非线性可分问题w难以稳定下来，不能求合适的解，如下图D。<br><img src="/images/pasted-41.png" alt="upload successful"><br>要解决非线性可分问题，需要考虑使用多层功能神经元<br><img src="/images/pasted-42.png" alt="upload successful"><br><img src="/images/pasted-43.png" alt="upload successful"><br>网络结构中，输入层与输出层之间的神经元层成为隐含层（hidden layer），每层神经元与下一层神经元完全互联，神经元之间不存在同层连接，也不存在跨层连接，称为多层前馈网络结构(multi-layer feedforward nerual networks)</p>
<ul>
<li>多层网络：包含隐层的网络</li>
<li>前馈网络：神经元之间不存在同层连接也不存在跨层连接  </li>
</ul>
<p>隐层和输出层具有激活函数，所以这两层的神经元亦称“功能单元”。多层前馈网络有强大的表示能力。只需一个包含足够多神经元的隐层，多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数。设置隐层神经元数，通常用“试错法”。</p>
<ul>
<li>主要特点：信号是前向传播的，而误差是反向传播的。</li>
<li>主要过程：信号的前向传播，从输入层经过隐含层，最后到达输出层</li>
<li>误差的反向传播，从输出层到隐含层，最后到输入层，依次调节隐含层到输出层的权重和偏置，输入层到隐含层的权重和偏置</li>
</ul>
<h2 id="误差逆传播算法——BP神经网络"><a href="#误差逆传播算法——BP神经网络" class="headerlink" title="误差逆传播算法——BP神经网络"></a>误差逆传播算法——BP神经网络</h2><p> 误差逆传播（error BackPropagation，简称BP）它是迄今为止最成功的神经网络学习算法，现实任务中使用神经网络时，大多在使用BP算法进行训练多层前馈神经网络，还可用于训练例如递归神经网络。</p>
<h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><p>$y=g(x)$，$z=h(y)$，$$\nabla{x}→\nabla{y}→\nabla{z} , \frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}$$<br>$x=g(s)$，$y=h(s)$，$z=k(x,y)$，$$\nabla{s}→\nabla{x},\nabla{y}→\nabla{z} , \frac{dz}{ds}=\frac{dz}{dx}\frac{dx}{ds}+\frac{dz}{dy}\frac{dy}{ds}$$</p>
<h3 id="BP算法过程"><a href="#BP算法过程" class="headerlink" title="BP算法过程"></a>BP算法过程</h3><p>给定训练集：$D=((x_1,y_1),(x_2,y_2)….(x_m,y_m)),x∈\Bbb{R}^d,y∈\Bbb{R}^l,$<br>输入：d维特征向量，（d个属性）；<br>输出：L个输出值（l维实值向量）；<br>隐层：假定使用q个隐层神经元；<br>输出层权重：$w_{ij}$；隐层权重：$v_{ij}$；输出层阈值：$θ_i$；隐层阈值：$γ_i$<br>隐层输入<br><img src="/images/pasted-45.png" alt="upload successful">；<br>输出层输入<br><img src="/images/pasted-46.png" alt="upload successful">；<br>隐层第h个神经元输出：bh；<br>假定功能单元均使用Sigmoid函数 。<br><img src="/images/pasted-47.png" alt="upload successful"><br>对训练<br><img src="/images/pasted-48.png" alt="upload successful">，假定输出为<br><img src="/images/pasted-49.png" alt="upload successful"> ，即<br><img src="/images/pasted-50.png" alt="upload successful">，则网络在<br><img src="/images/pasted-51.png" alt="upload successful">的均方误差为<br><img src="/images/pasted-52.png" alt="upload successful">，未知的参数包括隐层及输出层权值、阈值。<br>BP通过迭代学习，在每一轮采用广义的感知机学习规划对参数进行更新估计：<br><img src="/images/pasted-53.png" alt="upload successful">。BP算法基于梯度下降策略（gradient descent），以目标负梯度方向对参数进行调整。对于误差Ek，给定学习率：η：</p>
<p><img src="/images/pasted-54.png" alt="upload successful"><br>由于<br><img src="/images/pasted-55.png" alt="upload successful">，可得到<br><img src="/images/pasted-56.png" alt="upload successful"> 。<br>Sigmoid函数有以下性质：<br><img src="/images/pasted-57.png" alt="upload successful"> ，所以<br><img src="/images/pasted-58.png" alt="upload successful">：<br><img src="/images/pasted-59.png" alt="upload successful"><br>最终推得：<br><img src="/images/pasted-60.png" alt="upload successful"><br>其他参数的推导式同样的方法：<br><img src="/images/pasted-61.png" alt="upload successful"> 。<br>其中：<br><img src="/images/pasted-62.png" alt="upload successful"><br>学习率<br><img src="/images/pasted-63.png" alt="upload successful">，控制迭代中的更新步长，太大容易震荡，太小则收敛过慢。其中wθ与vγ的学习率不一定相等。</p>
<h3 id="BP算法流程"><a href="#BP算法流程" class="headerlink" title="BP算法流程"></a>BP算法流程</h3><p>算法的工作流程：<br><img src="/images/pasted-64.png" alt="upload successful"></p>
<h3 id="标准BP算法与累计BP算法"><a href="#标准BP算法与累计BP算法" class="headerlink" title="标准BP算法与累计BP算法"></a>标准BP算法与累计BP算法</h3><p>主要目标：最小化训练集D上的累计误差 。前面算法更新规则是基于单个Ek推导的，也称作“标准BP算法”。若使用基于累计误差最小化的更新规则，成为累计误差逆传播算法（accumulated errror backpropagation）。两者都很常用：</p>
<p>| ————- |—–:|<br>|标准BP算法|    1、每次针对单个训练样例更新权值与阈值；2、参数更新频繁，不同样例可以抵消，需要多次迭代|</p>
<p>|累计BP算法|    1、其优化目标是最小化整个训练集上的累计误差；<br>2、读取整个训练集一遍才对参数进行更新，参数更新频率较低|</p>
<p>累计BP算法更新频率低，防止不同样例导致训练出现抵消的现象。在很多任务中，累计误差下降到一定程度后，进一步下降会非常缓慢，这是标准BP算法往往会获得较好的解，尤其当训练集非常大时效果更明显。</p>
<h3 id="缓解过拟合"><a href="#缓解过拟合" class="headerlink" title="缓解过拟合"></a>缓解过拟合</h3><p>主要策略</p>
<ul>
<li>早停early stopping<br>将训练数据分为训练集和验证集。训练集计算梯度和更新，验证估计误差。<br>1、若训练误差连续a轮的变化小于b,则停止训练<br>2、使用验证集：若训练误差降低，验证误差升高，则停止训练。<br>返回具有最小验证误差的链接权重和阈值。</li>
<li>正则化<br>regularization    在误差目标函数中，增加一项描述网络复杂度：例如连接权和阈值的平方和<br>误差目标函数改为： ， 用于对经验误差和网络复杂度进行折中。偏好比较小的连接权和阈值，使网络输出更“光滑”</li>
</ul>
<h2 id="全局最小与局部极小"><a href="#全局最小与局部极小" class="headerlink" title="全局最小与局部极小"></a>全局最小与局部极小</h2><p><img src="/images/pasted-65.png" alt="upload successful"><br>神经网络的训练过程可看作一个参数寻优过程：<br>在参数空间中，寻找一组最优参数使得误差最小<br>特点：存在多个“局部极小”；只有一个“全局最小”<br>常用策略跳出局部极小</p>
<ul>
<li>不同参数进行初始化    </li>
<li>模拟退火（simulated annealing）    以一定概率接收比当前解更差的结果，每部迭代中，接受次优解的概率随时间推移而降低。</li>
<li>随机梯度下降    计算梯度时增加随机因素，即使陷入局部极小也有机会跳出继续搜索。</li>
<li>遗传算法（genetic algorithms）    </li>
</ul>
<h2 id="其他常见神经网络模型"><a href="#其他常见神经网络模型" class="headerlink" title="其他常见神经网络模型"></a>其他常见神经网络模型</h2><h3 id="RBF网络"><a href="#RBF网络" class="headerlink" title="RBF网络"></a>RBF网络</h3><p>RBF（Radial Basis Function，径向基函数）网络在分类任务中除BP之外最常用的一种<br>•    单隐层前馈神经网络<br>•    使用径向基函数作为隐层神经元激活函数ρ： ，定义为样本x到数据中心ci之间欧式距离的单调函数，常用高斯径向基函数。 。ci表示隐层神经元对应的中心、wi表示权重。<br>•    输出层是隐层神经元输出的线性组合<br>训练RBF网络：<br>•    确定神经元中心ci，常用的方式包括随机采样、聚类等<br>•    利用BP算法等确定参数wi和βi。</p>
<h3 id="ART网络"><a href="#ART网络" class="headerlink" title="ART网络"></a>ART网络</h3><p>ART（Adaptive Resonance Theory，自适应谐振理论）竞争学习的代表，是一种常用的无监督学习策略。该策略网络输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活。其他神经元被抑制。包含比较层、识别层、识别阈值和重置模块。</p>
<h3 id="SOM网络"><a href="#SOM网络" class="headerlink" title="SOM网络"></a>SOM网络</h3><p>SOM（Self-Organizing Map，自组织映射）网络是最常用的聚类方法之一：<br>•    竞争型的无监督神经网络<br>•    将高维数据映射到低维空间，并保持输入数据在高维空间的拓扑结构。即将高维空间中相似的样本点映射到网络输出层中邻近神经元<br>•    每个神经元拥有一个权向量<br>•    目标：为每个输出层神经元找到合适的权向量以保持拓扑结构<br>训练<br>•    网络接收输入样本后，将会确定输出层的“获胜”神经元（“胜者通吃”）<br>•    获胜神经元的权向量将向当前输入样本移动</p>
<h3 id="级联相关网络：“构造性”神经网络的代表"><a href="#级联相关网络：“构造性”神经网络的代表" class="headerlink" title="级联相关网络：“构造性”神经网络的代表"></a>级联相关网络：“构造性”神经网络的代表</h3><p>构造性神经网络：将网络结构也当做学习的目标，并在训练过程中找到最符合的网络结构。是结构自适应网络的重要代表。<br>训练<br>•    开始时只有输入层和输出层<br>•    级联（Cascade）：新的隐层节点逐渐加入，从而创建起层级结构<br>•    相关（Correlation）：最大化新节点的输出与网络误差之间的相关性</p>
<h3 id="Elman网络：递归神经网络的代表"><a href="#Elman网络：递归神经网络的代表" class="headerlink" title="Elman网络：递归神经网络的代表"></a>Elman网络：递归神经网络的代表</h3><p>•    网络可以有环形结构，可让使一些神经元的输出反馈回来最为输入<br>•    t 时刻网络的输出状态： 由 t 时刻的输入状态和 t-1时刻的网络状态共同决定<br>Elman网络是最常用的递归神经网络之一<br>•    结构与前馈神经网络很相似，但隐层神经元的输出被反馈回来<br>•    使用推广的BP算法训练</p>
<h3 id="Bolyzmann机：”基于能量的模型”的代表"><a href="#Bolyzmann机：”基于能量的模型”的代表" class="headerlink" title="Bolyzmann机：”基于能量的模型”的代表"></a>Bolyzmann机：”基于能量的模型”的代表</h3><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p>•    卷积神经网络CNN</p>
<p><img src="/images/pasted-66.png" alt="upload successful"><br>•    每个卷积层包含多个特征映射，每个特征映射是一个由多个神经元构成的“平面”，通过一种卷积滤波器提取输入的一种特征<br>•    采样层亦称“汇合层”，其作用是基于局部相关性原理进行亚采样，从而在减少数据量的同事保留有用信息<br>•    连接层就是传统神经网络对隐层与输出层的全连接<br>典型的深度学习模型就是很深层的神经网络</p>

                    
                        


                    
                    
                        <p>
                            <a
                                href="/2018/05/23/神经网络/#post-footer"
                                class="postShorten-excerpt_link link"
                                aria-label=""
                            >
                                Kommentieren und teilen
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a
                            class="link-unstyled"
                            href="/2018/05/16/贝叶斯/"
                            aria-label=": 贝叶斯分类器"
                        >
                            贝叶斯分类器
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-05-16T14:16:00+08:00">
	
		    16 5月 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/机器学习/">机器学习</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul>
<li><strong>条件概率的概念：</strong>A和B是两个事件，事件A发生的条件下，事件B发生的概率，表示为：$P(A|B)=P(AB)/P(B)$。<br>事件独立：$P(AB)=P(A)P(B)$。<br>求和$P(A+B)=P(A)+P(B)-P(AB)$。<br>求差$P(A-B)=P(A)-P(AB)$。<br>乘积：$P(AB)=P(A)P(B|A)=P(B)P(A|B)$。<br>联合分布：$\sum_{A} P(A,B)=P(B)$<br>$P(B)=\sum_{B|A}P(A)$<br>联合分布条件概率：$P(S|W)=\frac {P(S,W)}{P(W)}$<br>$P(A,B|C)=P(A|B,C)P(B|C)$</li>
<li><strong>贝叶斯公式：</strong><br>$P(B│A)=\frac{P(B)P(A|B)}{P(A)}$。<br>由全概率公式，当$B1、B2、B3……∈B，A⊂∑B_i$时，$P(B_i│A)=P(B_i)P(A|B_i)/(∑P(B_j)P(A|B_j))$。<ul>
<li>$P(B_i)$先验概率</li>
<li>$P(B_i│A)$为后验概率。</li>
</ul>
</li>
</ul>
<p>贝叶斯分类器是利用概率的知识完成数据的分类任务，在机器学习中使用贝叶斯决策论实施决策的基本方法也是在概率的框架下进行的，它是考虑如何基于这些概率和误判损失来选择最优的类别标记。</p>
<h2 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h2><p>假设有N种可能的类别标记$y=(c_1,c_2….c_N)$，$λ_{ij}$是将一个真实标记为$c_j$的样本误分类为$c_i$所产生的损失。基于后验概率$P(c_i|x)$可获得奖样本x分类为$c_i$所产生的期望损失，即在样本x上的“条件风险”。<br>$$R(c_i|x=\sum_{j=1}^Nλ_{ij}P(c_j|x))$$<br>机器学习的过程就是要寻找一个判定准则：$h:x–&gt;y$以最小化总体风险。<br>为最小化总体风险，只需要在每个样本上选择哪个能使条件分析最小的类别标记。<br>$$h^<em>(x)=arg min R(c|x)$$<br>此时，$h^</em>$称为贝叶斯最优分类器，与之对应的总体风险$R(h^<em>)$称为贝叶斯风险。则$1-R(h^</em>)$反映了分类器所能达到的最好性能。 </p>
<ul>
<li>生成式模型： 先对联合概率分布P(x,c)建模，然后由此获得P(c|x)，如：贝叶斯分类器。</li>
<li>判别式模型： 给定x，通过直接建模P(c|x)，预测c。如：决策树、BP神经网络、支持向量机</li>
</ul>
<h2 id="极大拟然估计"><a href="#极大拟然估计" class="headerlink" title="极大拟然估计"></a>极大拟然估计</h2><p>概率模型的训练过程就是参数估计过程，采用极大拟然估计就是试图在所有的可能的取值中，找到一个能使数据出现的“可能性”的最大值。  </p>
<ul>
<li>频率主义学派（Frequentist）：参数虽然未知，但是客观存在固定值，所以可以通过优化似然函数等准则确定参数值。极大似然估计（Maximum Likelihood Estimation，MLE）是根据数据采样来估计概率分布的。  </li>
<li>贝叶斯学派（Bayesian）：参数是未观察到的随机变量，其本身也可有分布，因此，可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。</li>
</ul>
<h2 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h2><p>朴素贝叶斯分类器（Naïve Bayes classifier）：基于贝叶斯公式来估计后验概率 的主要困难在于求类条件概率 是所有属性上的联合概率，难以从有限的训练样本直接估计得到。为了解决这个问题，提出朴素贝叶斯分类器 它采用了“属性条件独立假设”对已知类别，假设所有属性相互独立，换言之，假设每个属性独立地对分类结果发生影响。<br><img src="/images/pasted-79.png" alt="upload successful"><br><img src="/images/pasted-80.png" alt="upload successful"></p>
<h2 id="半朴素贝叶斯分类器"><a href="#半朴素贝叶斯分类器" class="headerlink" title="半朴素贝叶斯分类器"></a>半朴素贝叶斯分类器</h2><p>为了降低贝叶斯公式中估计后验概率$P(c|x)$的困难，提出使用朴素贝叶斯分类器采用属性条件独立性假设，然而在现实任务中这个假设很难成立，因此就提出半朴素贝叶斯分类器（Sem-naïve Bayes classifiers），它的基本思想是适当考虑一部分属性间的相互依赖信息，从而既不需要完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。<br>独依赖估计（One-Dependent Estimator，ODE）每个属性在类别之外最多仅依赖于一个其他属性。最直接的做法，假设所有属性都依赖于同一属性，称为为超父（super-parent ODE）。TAN（Tree Augmented naïve Bayes）使用最大生成树算法，将属性间依赖关系生成最大带权生成树，保留了强相关属性之间的依赖性。AODE（Average的 One-Dependent Estimator）基于集成学习机制，尝试将每个属性作为超父来构建SPODE，最终返回具有足够支撑的SPODE集成作为最终结果。<br><img src="/images/pasted-81.png" alt="upload successful"></p>
<h2 id="贝叶斯网"><a href="#贝叶斯网" class="headerlink" title="贝叶斯网"></a>贝叶斯网</h2><p>贝叶斯网是借助有向无环图（DAG）来刻画属性之间的依赖关系，并使用条件概率表（CPT）来描述属性的联合概率分布<br>结构<br>学习<br>推断<br><img src="/images/pasted-82.png" alt="upload successful"></p>
<h2 id="经典贝叶斯问题"><a href="#经典贝叶斯问题" class="headerlink" title="经典贝叶斯问题"></a>经典贝叶斯问题</h2><p>A女士怀疑自己得了某种肝炎，希望在医院做一次检测。医生告诉A女士，她所属的人群得此种肝炎的概率仅有千分之一。但A女士不放心，还是坚持做了测试。然而很不幸，测试结果为阳性。现在已知测试仪器的正确率为95%，那么A女士确实得了肝炎而非误诊的概率为多少？</p>

                    
                        


                    
                    
                        <p>
                            <a
                                href="/2018/05/16/贝叶斯/#post-footer"
                                class="postShorten-excerpt_link link"
                                aria-label=""
                            >
                                Kommentieren und teilen
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a
                            class="link-unstyled"
                            href="/2018/05/10/SVM/"
                            aria-label=": SVM"
                        >
                            SVM
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-05-10T15:50:00+08:00">
	
		    10 5月 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/机器学习/">机器学习</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><h3 id="间隔与支持间隔"><a href="#间隔与支持间隔" class="headerlink" title="间隔与支持间隔"></a>间隔与支持间隔</h3><p>分类学习的最基本想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。<br>能将训练样本划分开的平面可能有很多个，选择位于两类训练样本正中间的划分超平面，原因是这个超平面的分类结果最鲁棒，泛化能力最强。<br>在样本空间中，划分超平面可通过以下线性方程来描述<br>$$ω^Tx+b=0$$<br>样本空间中任意点 x 到超平面 (w,b) 的距离可以写成<br>$$r=\frac{|w^Tx+b|}{||w||}$$<br>假设超平面能够正确分类样本，则可以通过对 ω 缩放可以使得下式成立<br>\begin{cases}<br>|w^Tx+b|&gt;=1,y_i=+1<br>|w^Tx+b|&lt;=-1,y_i=-1<br>\end{cases}<br>距离超平面最近的几个样本点使得上式等号成立，称作“支持向量”。两个异类支持向量到超平面的距离之和称为“间隔”，$γ=\frac{2}{||w||}$。所有位于最大边界上的点称作支持向量。撑起了边界的宽度，其中$||w||$是向量的范数（norm）：<br>$$\if W = {w_1,w_2,….w_n}then\sqrt{W·W}=\sqrt{w_1^2,w_2^2,….w_n^2}$$<br><img src="/images/pasted-27.png" alt="upload successful"><br>距离是x和x’在w上的投影：<br><img src="/images/pasted-28.png" alt="upload successful"><br>欲最大化间隔，等价于最小化$||w||^2$ , 这就是支持向量机的基本型。<br>$$\min{w,b} \frac{1}{2} ||w||^2$$<br>$$s.t. y_i(w^Tx_i+b)≥1，i=1,2,…,m$$<br><img src="/images/pasted-29.png" alt="upload successful"></p>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>上述问题是一个凸二次规划问题，能直接用现成的优化计算包求解。但是通过拉格朗日乘子法变换到对偶变量的优化问题之后，可以找到一种更加有效的方法来进行求解。<br>原问题的拉格朗日函数为<br>$$L(w,b,α)=\frac{1}{2}||w||^2+\sum{i=1}^m α_i(1-y_i(w^Tx_i+b))$$<br>求偏导为零可以得到<br>$$w=\sum{i=1}^m α_iy_ix_i$$<br>$$0=\sum{i=1}^mα_iy_i$$<br>对偶问题，将最大最小互换：<br>$$\min{w,b}\max{α}L(w,b,α)-&gt;\max{α}\min{w,b}L(w,b,α)$$<br>$$\max{α} \sum{i=1}{m}α_i-\frac{1}{2}\sum{i=1}^m\sum{j=1}^mα_iα_jy_iy_jx_i^Tx_j{}$$<br>$$s.t. \sum{i=1}^mα_iy_i=0,α_i≥0，i=1,2,…,m$$<br>于是可以得到模型为<br>$$f(x)=w^Tx+b=\sum{i=1}^mα_iy_ix_i^Tx+b$$<br>上述过程需要满足KKT(Karush-Kuhn-Tucker)条件，即<br>\begin{cases}<br>α_i≥0<br>y_if(x_i)-1≥0<br>α_i(y_if(x_i)-1)=0<br>\end{cases}<br>对任意训练样本 $(x_i,y_i)$ ，总有 $α_i=0$ 或 $y_if(x_i)=1$ 。因此训练完成后，大部分的样本都不需要保留，最终模型仅与支持向量有关。<br>如果用二次规划算法求解对偶问题，则问题的规模正比于训练样本数，这会在实际任务中造成很大开销，为此提出SMO(Sequential Minimal Optimization)算法。</p>
<h4 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h4><p>SMO将比较大的问题拆解为比较小的问题，多个变量不好求，先求两个。<br>步骤：不断执行以下两个步骤直到收敛<br>1、选取一对需要更新的变量 αi 和 αj<br>2、固定 αi 和 αj 以外的参数，求解对偶问题更新后的 αi 和 αj<br>只要选取的 αi 和 αj 中有一个不满足KKT条件， 目标函数就会在迭代后减小。KKT条件违背的程度越大，变量更新后可能导致的目标函数值减幅越大。<br>使选取的两变量所对应样本之间的间隔最大（两个变量有很大的差别，对它们进行更新会带给目标函数值更大的变化）。</p>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>原始样本空间线性不可分：将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。如果原始空间是有限维，那么一定存在一个高维特征空间使样本可分。<br>模型变成：$ω^Tϕ(x)+b=0 $<br>$$\min{w,b}\frac{1}{2}||w||^2$$<br>$$s.t. y_i(w^Tϕ(x_i)+b)≥1，i=1,2,…m$$<br>对偶问题为：<br>$$\max{α} \sum{i=1}{m}α_i-\frac{1}{2}\sum{i=1}^m\sum{j=1}^mα_iα_jy_iy_jx_i^Tx_j{}$$<br>$$s.t. \sum{i=1}^mα_iy_i=0,α_i≥0，i=1,2,…,m$$<br>由于特征空间维数可能很高，直接计算 $ϕ(x_i)^Tϕ(x_j)$通常是困难的。设想函数$k(x_i,x_j)=ϕ(x_i)Tϕ(x_j)$,$x_ixi$与$x_jxj$在特征空间的内积<strong>等于</strong>它们在原始样本空间中通过核函数计算的结果。<br>核函数选择成为支持向量机的最大变数，若核函数选择不合适，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。比如图像分类，一般使用高斯径向基核函数，因为需要超平面要非常的平滑。<br>常用核函数：<br><img src="/images/pasted-30.png" alt="upload successful"></p>
<h2 id="软间隔与正则化"><a href="#软间隔与正则化" class="headerlink" title="软间隔与正则化"></a>软间隔与正则化</h2><p>现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分，即便线性可分，也很难判定这个结果不是由于过拟合造成的。<br>缓解这个问题的一个方法是允许支持向量机在一些样本上出错，引入“软间隔”概念。允许某些样本不满足约束 $y_i(ω^Txi+b)≥1 $<br>优化目标可写成<br>$$\min{w,b}\frac{1}{2}||w||^2+C\sum{i=1}^mξ_i$$<br>其中 l0/1 是“0/1损失函数”<br> <img src="/images/pasted-11.png" alt="upload successful"><br>l0/1 非凸、非连续、数学性质不好，使得上式难以求解，因此人们用其他一些函数来代替它，称为“代替函数”。<br>常用的代替函数：<br><img src="/images/pasted-21.png" alt="upload successful"><br>常用的软间隔支持向量机：<br><img src="/images/pasted-31.png" alt="upload successful"><br>与硬间隔支持向量机相似，软间隔支持向量机也是一个二次规划问题，可以通过拉格朗日乘子法得到拉格朗日函数：<br><img src="/images/pasted-32.png" alt="upload successful"><br>求偏导为零可以得到<br><img src="/images/pasted-33.png" alt="upload successful"><br>对偶问题为<br><img src="/images/pasted-34.png" alt="upload successful"><br>上述过程需要满足KKT(Karush-Kuhn-Tucker)条件，即<br><img src="/images/pasted-35.png" alt="upload successful"><br>实际上支持向量机和对率回归的优化目标相近，通常情况下他们的性能相当。对率回归的优势主要对于其输出具有自然的概率意义，即在给出预测标记的同时也给了概率，而支持向量机的输出不具有概率意义，欲得到概率需要进行特殊处理；此外，对率回归能够直接用于多分类任务，支持向量机为此需要进行推广。另一方面，可以看出hinge损失函数有一块平摊的零区域，这使得支持向量机的解具有稀疏性，而对率损失是光滑的而单调递减函数，不能导出类似支持向量的概念。因此对率回归的解依赖于更多的训练样本，其预测开销大。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X=np.r_[np.random.randn(<span class="number">20</span>,<span class="number">2</span>)-[<span class="number">2</span>,<span class="number">2</span>],np.random.randn(<span class="number">20</span>,<span class="number">2</span>)+[<span class="number">2</span>,<span class="number">2</span>]]</span><br><span class="line">Y=[<span class="number">0</span>]*<span class="number">20</span>+[<span class="number">1</span>]*<span class="number">20</span></span><br><span class="line">clf=svm.SVC(kernel=<span class="string">'linear'</span>)</span><br><span class="line">clf.fit(X,Y)</span><br><span class="line">w=clf.coef_[<span class="number">0</span>]</span><br><span class="line">a=-w[<span class="number">0</span>]/w[<span class="number">1</span>]</span><br><span class="line">xx=np.linspace(<span class="number">-5</span>,<span class="number">5</span>)</span><br><span class="line">yy=a*xx-(clf.intercept_[<span class="number">0</span>])/w[<span class="number">1</span>]</span><br><span class="line">b=clf.support_vectors_[<span class="number">0</span>]</span><br><span class="line">yy_down=a*xx+b[<span class="number">1</span>]-a*b[<span class="number">0</span>]</span><br><span class="line">b=clf.support_vectors_[<span class="number">-1</span>]</span><br><span class="line">yy_up=a*xx+b[<span class="number">1</span>]-a*b[<span class="number">0</span>]</span><br><span class="line">pl.plot(xx,yy,<span class="string">'k-'</span>)</span><br><span class="line">pl.plot(xx,yy_down,<span class="string">'k--'</span>)</span><br><span class="line">pl.plot(xx,yy_up,<span class="string">'k--'</span>)</span><br><span class="line">pl.scatter(clf.support_vectors_[:,<span class="number">0</span>],clf.support_vectors_[:,<span class="number">1</span>])</span><br><span class="line">pl.scatter(X[:,<span class="number">0</span>],X[:,<span class="number">1</span>],c=Y,cmap=pl.cm.Paired)</span><br><span class="line">pl.show()</span><br></pre></td></tr></table></figure></p>

                    
                        


                    
                    
                        <p>
                            <a
                                href="/2018/05/10/SVM/#post-footer"
                                class="postShorten-excerpt_link link"
                                aria-label=""
                            >
                                Kommentieren und teilen
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
        
        <li class="pagination-number">Seite 1 von 1</li>
    </ul>
</div>

</section>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2020 hero576. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <h4 id="about-card-name">hero576</h4>
        
            <div id="about-card-bio"><p>author.bio</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>author.job</p>

            </div>
        
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/jquery.js"></script>
<script src="/assets/js/jquery.fancybox.js"></script>
<script src="/assets/js/thumbs.js"></script>
<script src="/assets/js/tranquilpeak.js"></script>
<!--SCRIPTS END-->





    </body>
</html>
