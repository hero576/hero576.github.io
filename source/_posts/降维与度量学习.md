title: 降维与度量学习
author: hero576
tags:
  - 机器学习
categories:
  - 机器学习
date: 2018-06-06 18:15:00
---
> 
<!-- more -->


降维在一些图像识别过程也经常被采用的一种分类算法，例如二维数据经过投影变为一维数据，从而更好的表征数据的特征，再进行识别。在前面章节中提到过LDA（线性判别分析）也可以当做一种简单降维处理。

## 低维嵌入
- 维数灾难:
- 缓解维数灾难方法：降维（维数约简），也就是通过某种数学变换将原始高维属性空间转变为一个低维“子空间”，在这个子空间中样本密度大幅提高，距离计算也变得更为容易。 

在很多时候，人们观测或收集到的数据样本虽然是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维嵌入。   
- 线性降维方法：基于线性变换来进行降维的方法。

## 主成分分析（PCA）
 以二维特征为例，两个特征之间可能存在线性关系的（例如这两个特征分别是运动的时速和秒速度），这样就造成了第二维信息是冗余的。PCA的目标是为了发现这种特征之间的线性关系，检测出这些线性关系，并且去除这线性关系。

定义两个特征之间的协方差：
$$σ_{jk}=\frac{1}{n-1}\sum_{i=1}^n(x_{ij}-\overline x_j)(x_{ik}-\overline x_k)$$

多个特征之间的协方差矩阵：
$$\sum = \frac{1}{n-1}((X-\overline{x})^T(X-\overline{x}))$$
where $\overline{x}=\frac{1}{n}\sum_{k=1}^n x_i$

根据协方差矩阵，求出特征值、特征向量，找到对应特征值较大的k个特征向量组合成为**变换矩阵**，说明这k个特征值在整个特征空间是比较重要的。通过矩阵乘法，我们就把原样本空间压缩了。

## SVD






