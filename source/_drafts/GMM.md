title: GMM
author: hero576
tags:
  - 机器学习
categories:
  - 机器学习
date: 2018-06-04 16:43:00
---
> 
<!-- more -->


## 信息论
### 基础知识
- 自信息：$I(x)=l\log p(x)$
- 互信息：已知x，y不确定度的减少量取值可正可负。$I(x;y)= \log \frac{p(x|y)}{p(x)}=\log \frac{p(xy)}{p(x)p(y)}=\log \frac{p(y|x)}{p(y)}=I(y;x)$

### 熵
在决策树中已经了解了熵是什么：
$H(X)=\sum_{x}p(x)I(x)=-\sum_{x}p(x)\log p(x)$
单位为比特，表示信息的不确定度。

### 联合熵
$H(XY)=\sum_{x,y}p(x,y)I(xy)=-\sum_{x,y}p(x,y)\log p(x,y)$
联合熵是XY上每个元素x，y的自信息量概率加权平均。
条件熵与联合熵的关系为：
$H(XY)=H(X)+H(Y|X)=H(Y)+H(X|Y)$

### 最大熵原理


## EM算法
若概率模型的变量都是观测变量，则可以直接使用极大似然估计。但是大部分应用中，含有隐变量，不能直接使用极大似然。EM算法就是针对隐变量模型参数的极大似然的估计方法。

## 高斯混合模型
![upload successful](/images/pasted-83.png)定义很好理解，高斯混合模型是一种混合模型，混合的基本分布是高斯分布而已。
![upload successful](/images/pasted-84.png)
这图显示了拥有三个高斯分量的一个维度的GMM是如何由其高斯分量叠加而成。
**GMM的各个高斯分量的系数之和必须为1**  
      GMM的定义本质上是一个概率密度函数。而概率密度函数在其作用域内的积分之和必然为1。GMM整体的概率密度函数是由若干个高斯分量的概率密度函数线性叠加而成的，而每一个高斯分量的概率密度函数的积分必然也是1，所以，要想GMM整体的概率密度积分为1，就必须对每一个高斯分量赋予一个其值不大于1的权重，并且权重之和为1。  
**求解GMM的EM算法隐变量的理解：**
      使用EM算法必须明确隐变量。求解GMM的时候设想观测数据x是这样产生的：首选依赖GMM的某个高斯分量的系数概率（因为系数取值在0~1之间，因此可以看做是一个概率取值）选择到这个高斯分量，然后根据这个被选择的高斯分量生成观测数据。然后隐变量就是某个高斯分量是否被选中：选中就为1，否则为0。
      按照这样的设想：隐变量是一个向量，并且这个向量中只有一个元素取值为1，其它的都是0。因为假设只有一个高斯分量被选中并产生观测数据。然而我们的GMM的一个观测数据在直观上应该是每个高斯分量都有产生，而不是由一个高斯分量单独生成，只是重要性不同（由系数控制）。
      接着，现在我们不知道GMM具体的参数值，想要根据观测数据去求解其参数。而GMM的参数是由各个高斯分量的参数再加上权值系数组成的。那么我们就先假定，如果这个观测值只是由其中一个高斯分量产生，去求解其中一个高斯分量的参数。我们假设不同的观测值都有一个产生自己的唯一归宿，就像K-means算法一样。然后在后面的迭代过程中，根据数据整体似然函数的优化过程，逐渐找到一个最优的分配方案。然而，不同于K-means算法的是，我们最终给出的只是某一个观测是由某一个高斯分量唯一生成的概率值，而不是确定下来的属于某一类。每个高斯分量其实都可以产生这个观测数据只是输出不同而已，即产生观测数据的概率不同。最后，根据每个高斯分量产生观测数据的可能性不同，结合其权值汇总出整个GMM产生这个观测数据的概率值。
- EM算法
如果训练样本的属性“不完整”，因此在这种存在“未观测”变量（隐变量）的情形下，如何进行模型参数估计呢？EM算法就是常用的估计参数隐变量的利器。   
X表示已观测变量集，Z表示隐变量集，θ表示模型参数，对θ求极大似然估计，最大化对数似然为：$LL(Θ|X,Z)=lnP(X,Z|Θ)$。由于Z是隐变量，无法直接求解，此时需求通过Z计算期望，来最大化以观测数据的对数边际似然（marginal likelihood） $LL(Θ|X)=lnP(X|Θ)=ln\sum_{Z}P(X,Z|Θ)$
EM 算法 简单来说，使用两个步骤交替计算。不取Z的期望，而是基于$Θ^t$计算隐变量Z的概率分布$P(Z|X,Θ^t)$：期望（E）步，利用当前估计的参数值来计算对数似然的期望值；二是最大化（M）步，寻找能使E步产生的似然期望最大化的参数值。然后，新得到的参数值重新被用于 步，……直至收敛到局部最优解。

1. 定义隐变量  
我们引入隐变量γjk，它的取值只能是1或者0。  
 取值为1：第j个观测变量来自第k个高斯分量
 取值为0：第j个观测变量不是来自第k个高斯分量
那么对于每一个观测数据yj都会对应于一个向量变量Γj={γj1,...,γjK}，那么有： 
∑k=1Kγjk=1
p(Γj)=∏k=1Kαkγjk
其中，K为GMM高斯分量的个数，αk为第k个高斯分量的权值。因为观测数据来自GMM的各个高斯分量相互独立，而αk刚好可以看做是观测数据来自第k个高斯分量的概率，因此可以直接通过连乘得到整个隐变量Γj的先验分布概率。
2. 得到完全数据的似然函数  
对于观测数据yj,当已知其是哪个高斯分量生成的之后，其服从的概率分布为： 
p(yj|γjk=1;Θ)=N(yj|μk,Σk)
由于观测数据从哪个高斯分量生成这个事件之间的相互独立的，因此可以写为： 
p(yj|Γj;Θ)=∏k=1KN(yj|μk,Σk)γjk
这样我们就得到了已知Γj的情况下单个观测数据的后验概率分布。结合之前得到的Γj的先验分布，则我们可以写出单个完全观测数据的似然函数为： 
p(yj,Γj;Θ)=∏k=1KαkγjkN(yj|μk,Σk)γjk
最终得到所有观测数据的完全数据似然函数为： 
p(y,Γj;Θ)=∏j=1N∏k=1KαkγjkN(yj|μk,Σk)γjk
取对数，得到对数似然函数为： 
lnp(y,Γj;Θ)=∑j=1N∑k=1K(γjklnαk+γjklnN(yj|μk,Σk))
3. 得到各个高斯分量的参数计算公式
首先，我们将上式中的lnN(yj|μk,Σk)根据单高斯的向量形式的概率密度函数的表达形式展开： 
lnN(yj|μk,Σk)=−D2ln(2π)−12ln|Σk|−12(yj−μk)TΣ−1k(yj−μk)
假设我们已经知道隐变量γjk的取值，对上面得到的似然函数分别对αk和Σk求偏导并且偏导结果为零，可以得到：
μk=∑Nj=1∑Kk=1γjkyj∑Nj=1∑Kk=1γjk
Σk=∑Nj=1∑Kk=1γjk(yj−μk)(yj−μk)T∑Nj=1∑Kk=1γjk
由于在上面两式的第二个求和符号是对k=1...K求和，而在求和过程中γjk只有以此取到1，其它都是0，因此上面两式可以简化为： 
μk=∑Nj=1γjkyj∑Nj=1γjk
Σk=∑Nj=1γjk(yj−μk)(yj−μk)T∑Nj=1γjk
现在参数空间中剩下一个αk还没有求。这是一个约束满足问题，因为必须满足约束ΣKk=1αk=1。我们使用拉格朗日乘子法结合似然函数和约束条件对αk求偏导，可以得到： 
αk=∑Nj=1γjk−λ
将上式的左右两边分别对k=1...K求和，可以得到： 
λ=−N
将λ代入，最终得到： 
αk=∑Nj=1γjkN
至此，我们在隐变量已知的情况下得到了GMM的三种类型参数的求解公式。
4. 得到隐变量的估计公式 
根据EM算法，现在我们需要通过当前参数的取值得到隐变量的估计公式也就是说隐变量的期望的表达形式。即如何求解E{γjk|y,Θ}。 
E{γjk|y,Θ}=P(γjk=1|y,Θ)
=P(γjk=1,yj|Θ)∑Kk=1P(γjk=1,yj|Θ)
=P(yj|γjk=1,Θ)P(γjk=1|Θ)∑Kk=1P(yj|γjk=1,Θ)P(γjk=1|Θ)
=αkN(yj|μk,Σk)∑Kk=1αkN(yj|μk,Σk)
5. 使用EM算法迭代进行参数求解 
熟悉EM算法的朋友应该已经可以从上面的推导中找到EM算法的E步和M步。