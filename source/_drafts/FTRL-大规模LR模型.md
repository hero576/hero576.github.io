title: FTRL-大规模LR模型
author: hero576
date: 2018-06-06 22:54:29
tags:
---
在实际场景中，往往会有很多的样本数据，模型的特征有时就决定了接受样本数据量的范围。对于千万级别的数据，SVM、聚类、贝叶斯可能几乎不能训练完成。而可以并行的算法，Bagging、RF就能缓解这些问题，神经网络天生就是并行的。还有就是简单的模型比如回归，计算复杂度很低，也可以处理大规模的数据。对LR、FM这类模型的参数学习，传统的学习算法是batch learning算法，它无法有效地处理大规模的数据集，也无法有效地处理大规模的在线数据流。这时，有效且高效的online learning算法显得尤为重要。

[FTRL（Follow the Regularized Leader）](https://wenku.baidu.com/view/6618fdc8844769eae109ed95.html)正是这样一种算法，融合了RDA算法能产生稀疏模型的特性和SGD算法能产生更有效模型的特性。它在处理诸如LR之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色，国内各大互联网公司都已将该算法应用到实际产品中。

### [FTRL与SGD算法的关系](http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/12/05/understanding-FTRL-algorithm)

SGD算法的迭代计算公式如下：
$$w_{t+1}=w_t−η_tg_t$$
其中 t 为迭代轮数，w是模型参数，g是loss function关于w的梯度，而η是学习率，它随着迭代轮数增多而递减。
FTRL算法的迭代算公式如下：
$$w_{t+1}=arg min_w(∑_{s=1}^tg_s⋅w+\frac{1}{2}∑_{s=1}^tσ_s||w−w_s||_2^2+λ_1||w||_1)$$
其中 t 为迭代轮数，w是模型参数，$σ_s$定义成$∑_{s=1}^tσ_s=\frac{1}{η_t}$，$λ_1$是L1正则化系数。

实际这两个公式是等价的，通过对FTRL求导取极值，得出w与g的关系与SGD是相同的。FTRL公式左边两项承担了SGD算法的功能，而最右边的一项承担的是得到稀疏模型的功能。FTRL算法融合了RDA算法能产生稀疏模型的特性和SGD算法能产生更有效模型的特性，也就是说能学习出有效的且稀疏的模型。


